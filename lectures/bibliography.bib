@misc{richard1988richard,
  title     = {Richard {{Feynman}}'s Blackboard at Time of His Death},
  url       = {http://archives-dc.library.caltech.edu/islandora/object/ct1:483},
  publisher = {{The Caltech Archives}},
  urldate   = {2019-12-30},
  date      = {1988},
  author    = {Feynman, Richard}
}

@book{eliasmith2013how,
  location     = {{New York, New York}},
  title        = {How to Build a Brain: {{A}} Neural Architecture for Biological Cognition},
  isbn         = {978-0-19-026212-9},
  pagetotal    = {456},
  series       = {Oxford {{Series}} on {{Cognitive Models}} and {{Architectures}}},
  publisher    = {{Oxford University Press}},
  date         = {2013},
  author       = {Eliasmith, Chris},
  organization = {Oxford University Press}
}

@book{eliasmith2003neural,
  location  = {{Cambridge, Massachusetts}},
  title     = {Neural Engineering: {{Computation}}, Representation, and Dynamics in Neurobiological Systems},
  isbn      = {978-0-262-55060-4},
  pagetotal = {380},
  publisher = {{MIT Press}},
  date      = {2003},
  author    = {Eliasmith, Chris and Anderson, Charles H.}
}

@article{bekolay2014nengo,
  title        = {Nengo: {{A Python}} Tool for Building Large-Scale Functional Brain Models},
  volume       = {7},
  abstract     = {Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results.},
  number       = {48},
  journaltitle = {Frontiers in Neuroinformatics},
  date         = {2014},
  author       = {Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron R. and Eliasmith, Chris}
}

@article{voelker2020short,
  title   = {A short letter on the dot product between rotated Fourier transforms},
  author  = {Voelker, Aaron R},
  journal = {arXiv preprint arXiv:2007.13462},
  year    = {2020}
}

@inproceedings{furlong2022fractional,
  title     = {Fractional Binding in Vector Symbolic Architectures as Quasi-Probability Statements},
  author    = {Furlong, P. Michael and Eliasmith, Chris},
  booktitle = {44th Annual Meeting of the Cognitive Science Society},
  year      = {2022},
  publisher = {Cognitive Science Society},
  pdf       = {/files/publications/furlong.2022.pdf},
  url       = {http://compneuro.uwaterloo.ca/files/publications/furlong.2022.pdf},
  abstract  = {Distributed vector representations are a key bridging point between connectionist and symbolic representations of cognition.  It is unclear how uncertainty should be modelled in systems using such representations.  One may place vector-valued distributions over vector representations, although that may assign non-zero probabilities to vector symbols that cannot occur.  In this paper we discuss how bundles of symbols in Vector Symbolic Architectures (VSAs) can be understood as defining an object that has a relationship to a probability distribution, and how statements in VSAs can be understood as being analogous to probabilistic statements. We sketch novel designs for networks that compute entropy and mutual information of VSA-represented distributions.  In this paper we restrict ourselves to operators proposed for Holographic Reduced Representations, and representing real-valued data. However, we suggest that the methods presented in this paper should translate to any VSA where the dot product between fractionally bound symbols induces a valid kernel. }
}

@article{voelker2021a,
  title     = {Simulating and Predicting Dynamical Systems With Spatial Semantic Pointers},
  abstract  = {While neural networks are highly effective at learning task-relevant representations from data, they typically do not learn representations with the kind of symbolic structure that is hypothesized to support high-level cognitive processes, nor do they naturally model such structures within problem domains that are continuous in space and time. To fill these gaps, this work exploits a method for defining vector representations that bind discrete (symbol-like) entities to points in continuous topological spaces in order to simulate and predict the behavior of a range of dynamical systems. These vector representations are spatial semantic pointers (SSPs), and we demonstrate that they can (1) be used to model dynamical systems involving multiple objects represented in a symbol-like manner and (2) be integrated with deep neural networks to predict the future of physical trajectories. These results help unify what have traditionally appeared to be disparate approaches in machine learning.},
  author    = {Aaron R. Voelker and Peter Blouw and Xuan Choo and Nicole Sandra-Yaffa Dumont and Terrence C. Stewart and Chris Eliasmith},
  url       = {https://doi.org/10.1162/neco_a_01410},
  pdf       = {/files/publications/voelker.2021a.pdf},
  doi       = {10.1162/neco_a_01410},
  journal   = {Neural Computation},
  year      = {2021},
  month     = {07},
  volume    = {33},
  number    = {8},
  pages     = {2033-2067},
  publisher = {MIT Press}
}


@mastersthesis{bekolay2011masters,
  title   = {Learning in large-scale spiking neural networks},
  volume  = {Master of Mathematics},
  year    = {2011},
  month   = {09/2011},
  pages   = {141},
  school  = {University of Waterloo},
  type    = {Master's Thesis},
  address = {Waterloo, ON},
  author  = {Trevor Bekolay},
  url     = {http://uwspace.uwaterloo.ca/handle/10012/6195}
}

@inproceedings{sorscher2019unified,
  title     = {A unified theory for the origin of grid cells through the lens of pattern formation},
  author    = {Sorscher, Ben and Mel, Gabriel and Ganguli, Surya and Ocko, Samuel},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019}
}

@article{hafting2005microstructure,
  title        = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  volume       = {436},
  issn         = {1476-4687},
  url          = {https://doi.org/10.1038/nature03721},
  doi          = {10.1038/nature03721},
  abstract     = {The ability to find one's way depends on neural algorithms that integrate information about place, distance and direction, but the implementation of these operations in cortical microcircuits is poorly understood. Here we show that the dorsocaudal medial entorhinal cortex (dMEC) contains a directionally oriented, topographically organized neural map of the spatial environment. Its key unit is the ‘grid cell’, which is activated whenever the animal's position coincides with any vertex of a regular grid of equilateral triangles spanning the surface of the environment. Grids of neighbouring cells share a common orientation and spacing, but their vertex locations (their phases) differ. The spacing and size of individual fields increase from dorsal to ventral dMEC. The map is anchored to external landmarks, but persists in their absence, suggesting that grid cells may be part of a generalized, path-integration-based map of the spatial environment.},
  number       = {7052},
  journaltitle = {Nature},
  shortjournal = {Nature},
  date         = {2005-08-01},
  pages        = {801-806},
  author       = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.}
}

@book{okeefe1978hippocampus,
  location  = {{Oxford, United Kingdom}},
  title     = {The {{Hippocampus}} as a {{Cognitive Map}}},
  isbn      = {0-19-857206-9},
  url       = {http://cognitivemap.net/},
  publisher = {{Oxford University Press}},
  date      = {1978},
  author    = {O'Keefe, John and Nadel, Lynn}
}

@article{sussillo2009generating,
  title        = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  volume       = {63},
  issn         = {0896-6273},
  url          = {http://www.sciencedirect.com/science/article/pii/S0896627309005479},
  doi          = {https://doi.org/10.1016/j.neuron.2009.07.018},
  abstract     = {Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  number       = {4},
  journaltitle = {Neuron},
  date         = {2009},
  pages        = {544-557},
  keywords     = {SYSNEURO},
  author       = {Sussillo, David and Abbott, Laurence F.}
}


@article{nicola2017supervised,
  title        = {Supervised Learning in Spiking Neural Networks with {{FORCE}} Training},
  volume       = {8},
  issn         = {2041-1723},
  url          = {https://doi.org/10.1038/s41467-017-01827-3},
  doi          = {10.1038/s41467-017-01827-3},
  abstract     = {Populations of neurons display an extraordinary diversity in the behaviors they affect and display. Machine learning techniques have recently emerged that allow us to create networks of model neurons that display behaviors of similar complexity. Here we demonstrate the direct applicability of one such technique, the FORCE method, to spiking neural networks. We train these networks to mimic dynamical systems, classify inputs, and store discrete sequences that correspond to the notes of a song. Finally, we use FORCE training to create two biologically motivated model circuits. One is inspired by the zebra finch and successfully reproduces songbird singing. The second network is motivated by the hippocampus and is trained to store and replay a movie scene. FORCE trained networks reproduce behaviors comparable in complexity to their inspired circuits and yield information not easily obtainable with other techniques, such as behavioral responses to pharmacological manipulations and spike timing statistics.},
  number       = {1},
  journaltitle = {Nature Communications},
  shortjournal = {Nature Communications},
  date         = {2017-12-20},
  pages        = {2208},
  author       = {Nicola, Wilten and Clopath, Claudia}
}

@article{boerlin2011spikebased,
  title        = {Spike-{{Based Population Coding}} and {{Working Memory}}},
  volume       = {7},
  url          = {https://doi.org/10.1371/journal.pcbi.1001080},
  doi          = {10.1371/journal.pcbi.1001080},
  abstract     = {Author Summary Most of our daily actions are subject to uncertainty. Behavioral studies have confirmed that humans handle this uncertainty in a statistically optimal manner. A key question then is what neural mechanisms underlie this optimality, i.e. how can neurons represent and compute with probability distributions. Previous approaches have proposed that probabilities are encoded in the firing rates of neural populations. However, such rate codes appear poorly suited to understand perception in a constantly changing environment. In particular, it is unclear how probabilistic computations could be implemented by biologically plausible spiking neurons. Here, we propose a network of spiking neurons that can optimally combine uncertain information from different sensory modalities and keep this information available for a long time. This implies that neural memories not only represent the most likely value of a stimulus but rather a whole probability distribution over it. Furthermore, our model suggests that each spike conveys new, essential information. Consequently, the observed variability of neural responses cannot simply be understood as noise but rather as a necessary consequence of optimal sensory integration. Our results therefore question strongly held beliefs about the nature of neural “signal” and “noise”.},
  number       = {2},
  journaltitle = {PLOS Computational Biology},
  date         = {2011-02},
  pages        = {1-18},
  author       = {Boerlin, Martin and Denève, Sophie},
  publisher    = {Public Library of Science}
}

@article{boerlin2013predictive,
  title        = {Predictive {{Coding}} of {{Dynamical Variables}} in {{Balanced Spiking Networks}}},
  volume       = {9},
  url          = {https://doi.org/10.1371/journal.pcbi.1003258},
  doi          = {10.1371/journal.pcbi.1003258},
  abstract     = {Author Summary Two observations about the cortex have puzzled and fascinated neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks representing information reliably and with a small number of spikes. To achieve such efficiency, spikes of individual neurons must communicate prediction errors about a common population-level signal, automatically resulting in balanced excitation and inhibition and highly variable neural responses. We illustrate our approach by focusing on the implementation of linear dynamical systems. Among other things, this allows us to construct a network of spiking neurons that can integrate input signals, yet is robust against many perturbations. Most importantly, our approach shows that neural variability cannot be equated to noise. Despite exhibiting the same single unit properties as other widely used network models, our balanced networks are orders of magnitudes more reliable. Our results suggest that the precision of cortical representations has been strongly underestimated.},
  number       = {11},
  journaltitle = {PLOS Computational Biology},
  date         = {2013-11},
  pages        = {1-16},
  author       = {Boerlin, Martin and Machens, Christian K. and Denève, Sophie},
  publisher    = {Public Library of Science}
}


@article{eliasmith2005unified,
  title        = {A Unified Approach to Building and Controlling Spiking Attractor Networks},
  volume       = {7},
  number       = {6},
  journaltitle = {Neural computation},
  date         = {2005},
  pages        = {1276-1314},
  author       = {Eliasmith, Chris},
  type         = {article}
}

@book{abbott2001theoretical,
  title     = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  isbn      = {978-0-262-04199-7},
  url       = {https://mitpress.mit.edu/books/theoretical-neuroscience},
  abstract  = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.
               
               The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  pagetotal = {480},
  series    = {Computational {{Neuroscience}}},
  publisher = {{MIT Press}},
  date      = {2001-10},
  author    = {Abbott, Laurence F. and Dayan, Peter}
}

@mastersthesis{stoeckel2015design,
  location    = {{Germany}},
  title       = {Design Space Exploration of Associative Memories Using Spiking Neurons with Respect to Neuromorphic Hardware Implementations},
  institution = {{Bielefeld University}},
  date        = {2015},
  author      = {Stöckel, Andreas}
}

@book{kandel2012principles,
  title     = {Principles of {{Neural Science}}},
  edition   = {5},
  publisher = {{McGraw-Hill Education}},
  date      = {2012},
  author    = {Kandel, E. and Schwartz, J. and Jessell, T. and Siegelbaum, S. and Hudspeth, A. J.}
}

@article{chedotal2010wiring,
  langid       = {english},
  title        = {Wiring the Brain: The Biology of Neuronal Guidance},
  volume       = {2},
  issn         = {1943-0264},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/20463002},
  doi          = {10.1101/cshperspect.a001917},
  abstract     = {The mammalian brain is the most complex organ in the body. It controls all aspects of our bodily functions and interprets the world around us through our senses. It defines us as human beings through our memories and our ability to plan for the future. Crucial to all these functions is how the brain is wired in order to perform these tasks. The basic map of brain wiring occurs during embryonic and postnatal development through a series of precisely orchestrated developmental events regulated by specific molecular mechanisms. Below we review the most important features of mammalian brain wiring derived from work in both mammals and in nonmammalian species. These mechanisms are highly conserved throughout evolution, simply becoming more complex in the mammalian brain. This fascinating area of biology is uncovering the essence of what makes the mammalian brain able to perform the everyday tasks we take for granted, as well as those which give us the ability for extraordinary achievement.},
  number       = {6},
  journaltitle = {Cold Spring Harbor perspectives in biology},
  shortjournal = {Cold Spring Harb Perspect Biol},
  date         = {2010-06},
  pages        = {a001917-a001917},
  keywords     = {Humans,Animals,Axons/physiology,Brain/*cytology/*physiology,Gene Expression Regulation/physiology,Mammals/*physiology,Neurons/*cytology/*physiology},
  author       = {Chédotal, Alain and Richards, Linda J},
  eprinttype   = {pubmed},
  eprint       = {20463002}
}

@article{macneil2011,
  title   = {Fine-tuning and the stability of recurrent neural networks},
  journal = {PLoS ONE},
  volume  = {6},
  year    = {2011},
  url     = {http://www.plosone.org/article/info\%3Adoi\%2F10.1371\%2Fjournal.pone.0022885},
  author  = {David MacNeil and Chris Eliasmith},
  pdf     = {http://compneuro.uwaterloo.ca/files/publications/macneil.2011.pdf}
}

@article{kanwisher1997fusiform,
  title        = {The Fusiform Face Area: {{A}} Module in Human Extrastriate Cortex Specialized for Face Perception},
  volume       = {17},
  issn         = {0270-6474},
  url          = {https://www.jneurosci.org/content/17/11/4302},
  doi          = {10.1523/JNEUROSCI.17-11-04302.1997},
  abstract     = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate face area also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area FF) that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is selectively involved in the perception of faces.},
  number       = {11},
  journaltitle = {Journal of Neuroscience},
  date         = {1997},
  pages        = {4302-4311},
  author       = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  eprint       = {https://www.jneurosci.org/content/17/11/4302.full.pdf},
  publisher    = {{Society for Neuroscience}}
}

@article{markram2012human,
  title        = {The {{Human Brain Project}}},
  volume       = {306},
  number       = {6},
  journaltitle = {Scientific American},
  date         = {2012},
  pages        = {50-55},
  author       = {Markram, Henry}
}

@article{merolla2014million,
  title        = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  volume       = {345},
  issn         = {0036-8075},
  url          = {https://science.sciencemag.org/content/345/6197/668},
  doi          = {10.1126/science.1254642},
  abstract     = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brains structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  number       = {6197},
  journaltitle = {Science},
  date         = {2014},
  pages        = {668-673},
  author       = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  eprint       = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  publisher    = {{American Association for the Advancement of Science}}
}

@article{morgan2011darpa,
  entrysubtype    = {newspaper},
  title           = {{{DARPA}} Shells out \$21m for {{IBM}} Cat Brain Chip},
  url             = {https://www.theregister.co.uk/2011/08/18/ibm_darpa_synapse_project/},
  journaltitle    = {The Register},
  journalsubtitle = {Science},
  urldate         = {2020-01-03},
  date            = {2011-08-18},
  author          = {Morgan, Timothy P.}
}

@article{komer2016unified,
  title        = {A Unified Theoretical Approach for Biological Cognition and Learning},
  volume       = {11},
  issn         = {2352-1546},
  url          = {http://www.sciencedirect.com/science/article/pii/S2352154616300651},
  doi          = {https://doi.org/10.1016/j.cobeha.2016.03.006},
  abstract     = {Large-scale neural models are needed in order to understand the biological underpinnings of complex cognitive behavior. Good methods for constructing such models should provide for: first, abstraction (analysis across levels of description); second, integration (incorporation of simpler models to build more complex ones); third, empirical contact (using and comparing to a wide variety of neural data); and fourth, account for the varieties of learning. In this review we evaluate three prominent recent methods for constructing neural models using these four criteria. Each of these methods is being actively developed and demonstrates clear strengths along some of these criteria.},
  journaltitle = {Current Opinion in Behavioral Sciences},
  date         = {2016},
  pages        = {14-20},
  author       = {Komer, Brent and Eliasmith, Chris},
  note         = {Computational modeling}
}

@article{jonas2017could,
  title        = {Could a Neuroscientist Understand a Microprocessor?},
  volume       = {13},
  url          = {https://doi.org/10.1371/journal.pcbi.1005268},
  doi          = {10.1371/journal.pcbi.1005268},
  abstract     = {Author Summary Neuroscience is held back by the fact that it is hard to evaluate if a conclusion is correct; the complexity of the systems under study and their experimental inaccessability make the assessment of algorithmic and data analytic technqiues challenging at best. We thus argue for testing approaches using known artifacts, where the correct interpretation is known. Here we present a microprocessor platform as one such test case. We find that many approaches in neuroscience, when used naïvely, fall short of producing a meaningful understanding.},
  number       = {1},
  journaltitle = {PLOS Computational Biology},
  date         = {2017-01},
  pages        = {1-24},
  author       = {Jonas, Eric and Kording, Konrad Paul},
  publisher    = {{Public Library of Science}}
}

@article{boahen2017neuromorph,
  title        = {A Neuromorph's Prospectus},
  volume       = {19},
  issn         = {1521-9615},
  doi          = {10.1109/MCSE.2017.33},
  number       = {2},
  journaltitle = {Computing in Science Engineering},
  date         = {2017-03},
  pages        = {14-28},
  keywords     = {neuromorphic,computational neuroscience,neural engineering,Neuromorphics,Neural networks,Neuroscience,all-analog computing,all-digital computing,analog dendritic computation,analogue integrated circuits,asynchronous circuits,asynchronous digital circuits,asynchronous logic,autonomous robots,axons,cognitive computing,continuous signal encoding,dendrite emulation,digital axonal communication,Digital communication,digital computers,digital integrated circuits,electron traffic,embedded computing,Energy efficiency,error tolerance,ion channels,mapping arbitrary computations,mixed analog-digital systems,Moore's Law,Nanoscale devices,nanoscale dimensions,network-on-a-chip,neural networks,neurogrid neuromorphic system,neuromorph prospectus,neuromorphic chips,neuromorphic engineers,scientific computing,shrink transistors,single-lane nanoscale devices,spike trains,subthreshold analog circuits,subthreshold circuits,synaptic connections,system-on-a-chip,Three-dimensional displays,Transistors,trapped electrons--blocking lanes,Very large scale integration,VLSI},
  author       = {Boahen, Kwabena}
}

@book{splittgerber2018snell,
  title     = {Snell's {{Clinical Neuroanatomy}}},
  edition   = {8th Edition},
  abstract  = {Snell’s Clinical Neuroanatomy , Eighth Edition, equips medical and health professions students with a complete, clinically oriented understanding of neuroanatomy. Organized classically by system, this revised edition reflects the latest clinical approaches to neuroanatomy structures and reinforces concepts with enhanced, illustrations, diagnostic images, and surface anatomy photographs.
               
               Each chapter begins with clear objectives and a clinical case for a practical introduction to key concepts. Throughout the text, Clinical Notes highlight important clinical considerations.Chapters end with bulleted key concepts, along with clinical problem solving cases and review questions that test students’ comprehension and ensure preparation for clinical application.},
  pagetotal = {560},
  date      = {2018-11-06},
  author    = {Splittgerber, Ryan}
}

@misc{adee2009cat,
  title     = {Cat {{Fight Brews Over Cat Brain}}},
  url       = {https://spectrum.ieee.org/tech-talk/semiconductors/devices/blue-brain-project-leader-angry-about-cat-brain},
  abstract  = {Blue Brain’s Henry Markram says IBM lead researcher should be “strung up by the toes”},
  publisher = {{IEEE Spectrum}},
  urldate   = {2020-01-04},
  date      = {2009-11-23},
  author    = {Adee, Sally}
}

@book{reichert2000neurobiologie,
  langid    = {german},
  location  = {{Stuttgart [u.a.]}},
  title     = {Neurobiologie},
  edition   = {2., neubearb. und erw. Aufl.},
  isbn      = {3-13-745302-X},
  pagetotal = {VII, 250 S. : Ill., graph. Darst.},
  publisher = {{Thieme}},
  date      = {2000},
  keywords  = {Neurobiologie},
  author    = {Reichert, Heinrich}
}

@article{abbott1999lapicque,
  title        = {Lapicque’s Introduction of the Integrate-and-Fire Model Neuron (1907)},
  volume       = {50},
  issn         = {0361-9230},
  url          = {http://www.sciencedirect.com/science/article/pii/S0361923099001616},
  doi          = {https://doi.org/10.1016/S0361-9230(99)00161-6},
  number       = {5},
  journaltitle = {Brain Research Bulletin},
  date         = {1999},
  pages        = {303-304},
  author       = {Abbott, Laurence F.}
}

@article{lapicque1907recherches,
  langid       = {french},
  title        = {Recherches quantitatives sur l’excitation électrique des nerfs traitée comme une polarization},
  number       = {9},
  journaltitle = {Journal de Physiologie et de Pathologie Generale},
  date         = {1907},
  pages        = {620-635},
  author       = {Lapicque, Louis}
}

@article{hodgkin1952quantitative,
  title        = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  volume       = {117},
  number       = {4},
  journaltitle = {The Journal of Physiology},
  date         = {1952},
  pages        = {500-544},
  author       = {Hodgkin, Alan L. and Huxley, Andrew F.}
}


@book{traub1991neuronal,
  title     = {Neuronal Networks of the Hippocampus},
  volume    = {777},
  publisher = {{Cambridge University Press}},
  date      = {1991},
  author    = {Traub, Roger D. and Miles, Richard}
}

@book{rieke1999spikes,
  edition  = {Reprinted ed.},
  title    = {Spikes: {{Exploring}} the {{Neural Code}}},
  isbn     = {978-0-262-68108-7},
  abstract = {Our perception of the world is driven by input from the sensory nerves. This input arrives encoded as sequences of identical spikes. Much of neural computation involves processing these spike trains. What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons.The authors invite the reader to play the role of a hypothetical observer inside the brain who makes decisions based on the incoming spike trains. Rather than asking how a neuron responds to a given stimulus, the authors ask how the brain could make inferences about an unknown stimulus from a given neural response. The flavor of some problems faced by the organism is captured by analyzing the way in which the observer can make a running reconstruction of the sensory stimulus as it evolves in time. These ideas are illustrated by examples from experiments on several biological systems.Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by "real" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory. A quantitative framework is used to pose precise questions about the structure of the neural code. These questions in turn influence both the design and analysis of experiments on sensory neurons.},
  author   = {Rieke, Fred and Warland, David and Van Stevenick, Rob de Ruyter and Bialeck, William},
  month    = jul,
  year     = {1999}
}

@article{izhikevich2004model,
  title   = {Which Model to Use for Cortical Spiking Neurons?},
  volume  = {15},
  number  = {5},
  journal = {IEEE transactions on neural networks},
  author  = {Izhikevich, Eugene M.},
  year    = {2004},
  pages   = {1063-1070}
}

@article{izhikevich2003simple,
  title   = {Simple Model of Spiking Neurons},
  volume  = {14},
  number  = {6},
  journal = {IEEE Transactions on neural networks},
  author  = {Izhikevich, Eugene M.},
  year    = {2003},
  pages   = {1569-1572}
}

@article{hunsberger2015spiking,
  title        = {Spiking {{Deep Networks}} with {{LIF Neurons}}},
  url          = {http://arxiv.org/abs/1510.08829},
  abstract     = {We train spiking deep networks using leaky integrate-and-fire (LIF) neurons, and achieve state-of-the-art results for spiking networks on the CIFAR-10 and MNIST datasets. This demonstrates that biologically-plausible spiking LIF neurons can be integrated into deep networks can perform as well as other spiking models (e.g. integrate-and-fire). We achieved this result by softening the LIF response function, such that its derivative remains bounded, and by training the network with noise to provide robustness against the variability introduced by spikes. Our method is general and could be applied to other neuron types, including those used on modern neuromorphic hardware. Our work brings more biological realism into modern image classification models, with the hope that these models can inform how the brain performs this difficult task. It also provides new methods for training deep networks to run on neuromorphic hardware, with the aim of fast, power-efficient image classification for robotics applications.},
  journaltitle = {arXiv:1510.08829},
  date         = {2015},
  author       = {Hunsberger, Eric and Eliasmith, Chris}
}

@article{hubel1959receptive,
  title        = {Receptive {{Fields}} of {{Single Neurones}} in the {{Cat}}'s {{Striate Cortex}}},
  volume       = {148},
  url          = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1959.sp006308},
  doi          = {10.1113/jphysiol.1959.sp006308},
  number       = {3},
  journaltitle = {The Journal of physiology},
  date         = {1959},
  pages        = {574-591},
  author       = {Hubel, David H. and Wiesel, Torsten N.}
}


@misc{2009itur,
  title     = {{{ITU}}-{{R}}  {{M}}.1677-1: {{International Morse Code}}},
  date      = {2009-10},
  publisher = {{International Telecommunication Union}},
  url       = {https://www.itu.int/rec/R-REC-M.1677-1-200910-I/}
}

@misc{mann1997nervous,
  title   = {The {{Nervous System In Action}}},
  author  = {Mann, Michael D.},
  year    = {1997},
  url     = {http://michaeldmann.net/The\%20Nervous\%20System\%20In\%20Action.html},
  urldate = {2020-01-13}
}

@incollection{katsuki1969neural,
  title     = {Neural Mechanism of Auditory Sensation in Cats},
  booktitle = {Sensory Communication},
  author    = {Katsuki, Y.},
  editor    = {Rosenblith, W.A.},
  year      = {1969},
  publisher = {{MIT Press}},
  isbn      = {978-0-262-51842-0},
  lccn      = {2013370006}
}

@article{saleem2013integration,
  title    = {Integration of Visual Motion and Locomotion in Mouse Visual Cortex},
  author   = {Saleem, Aman B and Ayaz, Asl\i{} and Jeffery, Kathryn J and Harris, Kenneth D and Carandini, Matteo},
  year     = {2013},
  month    = dec,
  volume   = {16},
  pages    = {1864--1869},
  issn     = {1546-1726},
  doi      = {10.1038/nn.3567},
  abstract = {The primary visual cortex (V1) carries signals related to visual speed, and its responses are also affected by run speed. Here the authors report that nearly half of the V1 neurons were reliably driven by combinations of visual speed and run speed. As a population, V1 neurons predicted a linear combination of visual and run speed better than visual or run speeds alone.},
  journal  = {Nature Neuroscience},
  number   = {12}
}

@article{gabor1946theory,
  title        = {Theory of Communication. {{Part}} 1: {{The}} Analysis of Information},
  author       = {Gabor, Dennis},
  date         = {1946},
  journaltitle = {Journal of the Institution of Electrical Engineers-Part III: Radio and Communication Engineering},
  volume       = {93},
  pages        = {429--441},
  publisher    = {{IET}},
  number       = {26}
}

@book{gerstner2002spiking,
  title     = {Spiking {{Neuron Models}}: {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  author    = {Gerstner, Wulfram and Kistler, Werner M.},
  date      = {2002},
  publisher = {{Cambridge University Press}},
  isbn      = {978-0-521-89079-3},
  note      = {LCCB: 2002067657}
}

@article{sparks2002brainstem,
  title        = {The Brainstem Control of Saccadic Eye Movements},
  author       = {Sparks, David L.},
  date         = {2002-12-01},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nature Reviews Neuroscience},
  volume       = {3},
  pages        = {952--964},
  issn         = {1471-0048},
  doi          = {10.1038/nrn986},
  url          = {https://doi.org/10.1038/nrn986},
  abstract     = {The oculomotor system is a useful model for the study of purposeful movements. Rotations of the eyes are produced by three pairs of extraocular muscles that are innervated by motor neurons from the III, IV and VI cranial nerve nuclei. A saccadic eye movement is produced when the appropriate motor neurons produce a burst of spikes, followed by tonic firing at the correct rate to maintain the eye's new position. The commands for horizontal components of saccades originate in the pons and medulla. Omnipause neurons fire tonically during fixation, but stop firing during saccades; long-lead burst neurons and excitatory burst neurons fire before and during saccades. Excitatory burst neurons synapse onto motor neurons and drive the motor neuron pulse of activity. Neurons in the prepositus and vestibular nuclei fire tonically and drive the step of activity that maintains eye position. Microstimulation of neurons in the pontine reticular formation produces horizontal eye movements. Neurons in the rostral midbrain that have similar properties drive vertical eye movements. The duration and velocity of saccades are determined by the duration and maximal firing rate of the bursts of activity produced by these neurons. The horizontal and vertical components of oblique saccades are coordinated. The shorter of the two components proceeds at a lower velocity than normal so that the two will have the same duration and the movement will not be curved. The onsets of the two components are coordinated by omnipause neurons in the pons. The torsional component of saccadic eye movements is stereotyped and obeys Donders' law: for any direction of the line of sight, if the head is upright and stationary, the eye will assume a given degree of torsion, regardless of the route taken by the eye to reach its position. Saccades also obey Listing's law, which specifies the orientation of the globe for each gaze position. It is unclear how these laws are implemented, but identifying the mechanisms is an important part of understanding the generation of saccades. Neurons in the superior colliculus (SC) provide the main input to the pontine and midbrain pulse–step generators. Microstimulation of these neurons produces saccades in head-restrained animals, and coordinated head and eye movements in head-unrestrained animals. The size and direction of the movements produced are primarily determined by the position of the stimulation in the SC. Models of saccade generation assume that saccades are under feedback control and that the feedback comes from corollary discharge. Models differ in the implementation of this feedback, but it has not been possible to determine the type of feedback signal used or the site of the comparator. Reasons for this include the fact that signals that are separate in models might be intermingled in the brain, preventing selective lesioning of these signals, and that different models produce similar signals. Models also tend to use population signals, but in terms of electrophysiology, it is hard to know what the population signal is at any given time. The problems faced by oculomotor researchers are mirrored in other areas of systems neuroscience.},
  number       = {12}
}

@article{georgopoulos1982relations,
  title        = {On the Relations between the Direction of Two-Dimensional Arm Movements and Cell Discharge in Primate Motor Cortex},
  author       = {Georgopoulos, A P and Kalaska, J F and Caminiti, R and Massey, J T},
  date         = {1982-11},
  journaltitle = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  shortjournal = {J Neurosci},
  volume       = {2},
  pages        = {1527--1537},
  issn         = {0270-6474},
  doi          = {10.1523/JNEUROSCI.02-11-01527.1982},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/7143039},
  abstract     = {The activity of single cells in the motor cortex was recorded while monkeys made arm movements in eight directions (at 45 degrees intervals) in a two-dimensional apparatus. These movements started from the same point and were of the same amplitude. The activity of 606 cells related to proximal arm movements was examined in the task; 323 of the 606 cells were active in that task and were studied in detail. The frequency of discharge of 241 of the 323 cells (74.6\%) varied in an orderly fashion with the direction of movement. Discharge was most intense with movements in a preferred direction and was reduced gradually when movements were made in directions farther and farther away from the preferred one. This resulted in a bell-shaped directional tuning curve. These relations were observed for cell discharge during the reaction time, the movement time, and the period that preceded the earliest changes in the electromyographic activity (approximately 80 msec before movement onset). In about 75\% of the 241 directionally tuned cells, the frequency of discharge, D, was a sinusoidal function of the direction of movement, theta: D = b0 + b1 sin theta + b2cos theta, or, in terms of the preferred direction, theta 0: D = b0 + c1cos (theta - theta0), where b0, b1, b2, and c1 are regression coefficients. Preferred directions differed for different cells so that the tuning curves partially overlapped. The orderly variation of cell discharge with the direction of movement and the fact that cells related to only one of the eight directions of movement tested were rarely observed indicate that movements in a particular direction are not subserved by motor cortical cells uniquely related to that movement. It is suggested, instead, that a movement trajectory in a desired direction might be generated by the cooperation of cells with overlapping tuning curves. The nature of this hypothetical population code for movement direction remains to be elucidated.},
  eprint       = {7143039},
  eprinttype   = {pubmed},
  keywords     = {*Movement,Animals,Arm/physiology,Biomechanical Phenomena,Electromyography,Macaca mulatta,Male,Motor Cortex/*physiology,Neurons/physiology},
  langid       = {english},
  number       = {11}
}

@article{burke1967composite,
  title        = {Composite Nature of the Monosynaptic Excitatory Postsynaptic Potential.},
  author       = {Burke, R E},
  date         = {1967},
  journaltitle = {Journal of Neurophysiology},
  volume       = {30},
  pages        = {1114--1137},
  doi          = {10.1152/jn.1967.30.5.1114},
  url          = {https://doi.org/10.1152/jn.1967.30.5.1114},
  eprint       = {https://doi.org/10.1152/jn.1967.30.5.1114},
  number       = {5}
}

@article{voelker2018improving,
  title        = {Improving {{Spiking Dynamical Networks}}: {{Accurate Delays}}, {{Higher}}-{{Order Synapses}}, and {{Time Cells}}},
  author       = {Voelker, Aaron R. and Eliasmith, Chris},
  date         = {2018-03},
  journaltitle = {Neural Computation},
  volume       = {30},
  pages        = {569--609},
  publisher    = {MIT Press},
  doi          = {10.1162/neco_a_01046},
  url          = {https://www.mitpressjournals.org/doi/abs/10.1162/neco_a_01046},
  abstract     = {Researchers building spiking neural networks face the challenge of improving the biological plausibility of their model networks while maintaining the ability to quantitatively characterize network behavior. In this work, we extend the theory behind the neural engineering framework (NEF), a method of building spiking dynamical networks, to permit the use of a broad class of synapse models while maintaining prescribed dynamics up to a given order. This theory improves our understanding of how low-level synaptic properties alter the accuracy of high-level computations in spiking dynamical networks. For completeness, we provide characterizations for both continuous-time (i.e., analog) and discrete-time (i.e., digital) simulations. We demonstrate the utility of these extensions by mapping an optimal delay line onto various spiking dynamical networks using higher-order models of the synapse. We show that these networks nonlinearly encode rolling windows of input history, using a scale invariant representation, with accuracy depending on the frequency content of the input signal. Finally, we reveal that these methods provide a novel explanation of time cell responses during a delay task, which have been observed throughout hippocampus, striatum, and cortex.},
  number       = {3}
}


@inproceedings{voelker2019lmu,
  title     = {Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author    = {Voelker, Aaron R. and Kaji{\'{c}}, Ivana and Eliasmith, Chris},
  date      = {2019},
  abstract  = {We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize its continuous-time history – doing so by solving d coupled ordinary differential equations (ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree d - 1. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning 100000 time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time – exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network's disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using m recurrently-connected Poisson spiking neurons, ( m ) time and memory, with error scaling as ( d /  ). We discuss implementations of LMUs on analog and digital neuromorphic hardware.}
}

@thesis{voelker2019thesis,
  title       = {Dynamical Systems in Spiking Neuromorphic Hardware},
  author      = {Voelker, Aaron R.},
  date        = {2019},
  institution = {{University of Waterloo}},
  location    = {{Waterloo, ON}},
  url         = {http://hdl.handle.net/10012/14625},
  abstract    = {Dynamical systems are universal computers. They can perceive stimuli, remember, learn from feedback, plan sequences of actions, and coordinate complex behavioural responses. The Neural Engineering Framework (NEF) provides a general recipe to formulate models of such systems as coupled sets of nonlinear differential equations and compile them onto recurrently connected spiking neural networks – akin to a programming language for spiking models of computation. The Nengo software ecosystem supports the NEF and compiles such models onto neuromorphic hardware. In this thesis, we analyze the theory driving the success of the NEF, and expose several core principles underpinning its correctness, scalability, completeness, robustness, and extensibility. We also derive novel theoretical extensions to the framework that enable it to far more effectively leverage a wide variety of dynamics in digital hardware, and to exploit the device-level physics in analog hardware. At the same time, we propose a novel set of spiking algorithms that recruit an optimal nonlinear encoding of time, which we call the Delay Network (DN). Backpropagation across stacked layers of DNs dramatically outperforms stacked Long Short-Term Memory (LSTM) networks—a state-of-the-art deep recurrent architecture—in accuracy and training time, on a continuous-time memory task, and a chaotic time-series prediction benchmark. The basic component of this network is shown to function on state-of-the-art spiking neuromorphic hardware including Braindrop and Loihi. This implementation approaches the energy-efficiency of the human brain in the former case, and the precision of conventional computation in the latter case.},
  pdf         = {https://uwspace.uwaterloo.ca/bitstream/handle/10012/14625/Voelker\textsubscript{A}aron.pdf},
  type        = {PhD thesis}
}

@incollection{roth2009modeling,
  title     = {Modeling {{Synapses}}},
  booktitle = {Computational {{Modeling Methods}} for {{Neuroscientists}}},
  author    = {Roth, Arnd and van Rossum, Mark C. W.},
  editor    = {Schutter, Erik De},
  date      = {2009},
  pages     = {139--159},
  publisher = {{The MIT Press}}
}

@article{strata1999dale,
  title        = {Dale’s Principle},
  author       = {Strata, Piergiorgio and Harvey, Robin},
  date         = {1999},
  journaltitle = {Brain Research Bulletin},
  volume       = {50},
  pages        = {349--350},
  issn         = {0361-9230},
  doi          = {https://doi.org/10.1016/S0361-9230(99)00100-8},
  url          = {http://www.sciencedirect.com/science/article/pii/S0361923099001008},
  number       = {5}
}

@article{piantadosi2018one,
  title        = {One Parameter Is Always Enough},
  author       = {Piantadosi, Steven T.},
  date         = {2018},
  journaltitle = {AIP Advances},
  volume       = {8},
  pages        = {095118},
  doi          = {10.1063/1.5031956},
  url          = {https://doi.org/10.1063/1.5031956},
  eprint       = {https://doi.org/10.1063/1.5031956},
  number       = {9}
}

@thesis{hunsberger2018spiking,
  title       = {Spiking {{Deep Neural Networks}}: {{Engineered}} and {{Biological Approaches}} to {{Object Recognition}}},
  author      = {Hunsberger, Eric},
  date        = {2018},
  institution = {{University of Waterloo}},
  url         = {http://hdl.handle.net/10012/12819},
  abstract    = {Modern machine learning models are beginning to rival human performance on some realistic object recognition tasks, but we still lack a full understanding of how the human brain solves this same problem. This thesis combines knowledge from machine learning and computational neuroscience to create models of human object recognition that are increasingly realistic both in their treatment of low-level neural mechanisms and in their reproduction of high-level human behaviour. First, I present extensions to the Neural Engineering Framework to make its preferred type of model---the “fixed-encoding” network---more accurate for object recognition tasks. These extensions include better distributions---such as Gabor filters---for the encoding weights, and better loss functions---namely weighted squared loss, softmax loss, and hinge loss---to solve for decoding weights. Second, I introduce increased biological realism into deep convolutional neural networks trained with backpropagation, by training them to run using spiking leaky integrate-and-fire (LIF) neurons. These models have been successful in machine learning, and I am able to convert them to spiking networks while retaining similar levels of performance. I present a novel method to smooth the LIF rate response function in order to avoid the common problems associated with differentiating spiking neurons in general and LIF neurons in particular. I also derive a number of novel characterizations of spiking variability, and use these to train spiking networks to be more robust to this variability. Finally, to address the problems with implementing backpropagation in a biological system, I train spiking deep neural networks using the more biological Feedback Alignment algorithm. I examine this algorithm in depth, including many variations on the core algorithm, methods to train using non-differentiable spiking neurons, and some of the limitations of the algorithm. Using these findings, I construct a spiking model that learns online in a biologically realistic manner. The models developed in this thesis help to explain both how spiking neurons in the brain work together to allow us to recognize complex objects, and how the brain may learn this behaviour. Their spiking nature allows them to be implemented on highly efficient neuromorphic hardware, opening the door to object recognition on energy-limited devices such as cell phones and mobile robots.},
  type        = {PhD thesis}
}

@book{hebb1949organization,
  title     = {The {{Organization}} of {{Behavior}}: {{A Neuropsychological Theory}}},
  author    = {Hebb, Donald O.},
  date      = {1949},
  publisher = {{Wiley}},
  series    = {A {{Wiley}} Book in Clinical Psychology}
}

@article{sjostrom2010spiketiming,
  title        = {Spike-Timing Dependent Plasticity},
  author       = {Sjöström, J. and Gerstner, W.},
  date         = {2010},
  journaltitle = {Scholarpedia},
  volume       = {5},
  pages        = {1362},
  doi          = {10.4249/scholarpedia.1362},
  number       = {2}
}

@book{abbott2001theoretical,
  title     = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  author    = {Abbott, Laurence F. and Dayan, Peter},
  date      = {2001-10},
  publisher = {{MIT Press}},
  url       = {https://mitpress.mit.edu/books/theoretical-neuroscience},
  abstract  = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.
               
               The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  file      = {/home/andreas/Uni/PhD/bibliography/pdfs/books/Abbott and Dayan - 2001 - Theoretical Neuroscience Computational and Mathem.pdf},
  isbn      = {978-0-262-04199-7},
  pagetotal = {480},
  series    = {Computational {{Neuroscience}}}
}

@book{jackendoff2002foundations,
  title     = {Foundations of Language: {{Brain}}, Meaning, Grammar, Evolution},
  author    = {Jackendoff, Ray},
  date      = {2002},
  publisher = {{Oxford University Press, USA}},
  abstract  = {Already hailed as a masterpiece, Foundations of Language offers a brilliant overhaul of the last thirty-five years of research in generative linguistics and related fields. "Few books really deserve the cliché 'this should be read by every researcher in the field,'" writes Steven Pinker, author of The Language Instinct, "But Ray Jackendoff's Foundations of Language does."
               Foundations of Language offers a radically new understanding of how language, the brain, and perception intermesh. The book renews the promise of early generative linguistics: that language can be a valuable entree into understanding the human mind and brain. The approach is remarkably interdisciplinary. Behind its innovations is Jackendoff's fundamental proposal that the creativity of language derives from multiple parallel generative systems linked by interface components. This shift in basic architecture makes possible a radical reconception of mental grammar and how it is learned. As a consequence, Jackendoff is able to reintegrate linguistics with philosophy of mind, cognitive and developmental psychology, evolutionary biology, neuroscience, and computational linguistics. Among the major topics treated are language processing, the relation of language to perception, the innateness of language, and the evolution of the language capacity, as well as more standard issues in linguistic theory such as the roles of syntax and the lexicon. In addition, Jackendoff offers a sophisticated theory of semantics that incorporates insights from philosophy of language, logic and formal semantics, lexical semantics of various stripes, cognitive grammar, psycholinguistic and neurolinguistic approaches, and the author's own conceptual semantics.
               Here then is the most fundamental contribution to linguistic theory in over three decades.},
  isbn      = {978-0-19-926437-7},
  pagetotal = {504}
}

@article{hummel2003symbolicconnectionist,
  title        = {A Symbolic-Connectionist Theory of Relational Inference and Generalization.},
  author       = {Hummel, John E. and Holyoak, Keith J.},
  date         = {2003},
  journaltitle = {Psychological Review},
  volume       = {110},
  pages        = {220--264},
  publisher    = {{American Psychological Association}},
  issn         = {1939-1471(Electronic),0033-295X(Print)},
  doi          = {10.1037/0033-295X.110.2.220},
  abstract     = {The authors present a theory of how relational inference and generalization can be accomplished within a cognitive architecture that is psychologically and neurally realistic. Their proposal is a form of symbolic connectionism: a connectionist system based on distributed representations of concept meanings, using temporal synchrony to bind fillers and roles into relational structures. The authors present a specific instantiation of their theory in the form of a computer simulation model, Learning and Inference with Schemas and Analogies (LISA). By using a kind of self-supervised learning, LISA can make specific inferences and form new relational generalizations and can hence acquire new schemas by induction from examples. The authors demonstrate the sufficiency of the model by using it to simulate a body of empirical phenomena concerning analogical inference and relational generalization. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords     = {*Connectionism,*Generalization (Learning),*Inference,*Neural Networks,Analogy,Schema},
  number       = {2},
  place        = {US}
}

@article{vandervelde2006neural,
  title        = {Neural Blackboard Architectures of Combinatorial Structures in Cognition},
  author       = {van der Velde, Frank and de Kamps, Marc},
  date         = {2006},
  journaltitle = {Behavioral and Brain Sciences},
  volume       = {29},
  pages        = {37--70},
  publisher    = {{Cambridge University Press}},
  doi          = {10.1017/S0140525X06009022},
  number       = {1},
  options      = {useprefix=true}
}

@article{smolensky1990tensor,
  title        = {Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems},
  author       = {Smolensky, Paul},
  date         = {1990},
  journaltitle = {Artificial Intelligence},
  volume       = {46},
  pages        = {159--216},
  issn         = {0004-3702},
  doi          = {https://doi.org/10.1016/0004-3702(90)90007-M},
  url          = {http://www.sciencedirect.com/science/article/pii/000437029090007M},
  abstract     = {A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks.},
  number       = {1}
}

@inproceedings{gayler2003vector,
  title      = {Vector {{Symbolic Architectures}} Answer {{Jackendoff}}'s Challenges for Cognitive Neuroscience},
  booktitle  = {Proceedings of the {{ICCS}}/{{ASCS International Conference}} on {{Cognitive Science}}},
  author     = {Gayler, Ross},
  date       = {2003},
  pages      = {133--138},
  location   = {{Sydney, Australia: University of New South Wales}},
  eventtitle = {{{ICCS}}/{{ASCS International Conference}} on {{Cognitive Science}}}
}


@article{plate1995holographic,
  title        = {Holographic Reduced Representations},
  author       = {Plate, Tony A.},
  date         = {1995-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume       = {6},
  pages        = {623--641},
  issn         = {1941-0093},
  doi          = {10.1109/72.377968},
  keywords     = {Artificial intelligence,associative memory,Associative memory,associative processing,Cancer,circular convolution,compositional structures,Concrete,content-addressable storage,convolution,Convolution,Councils,Degradation,holographic reduced representations,holographic storage,Holography,neural nets,Tree data structures,vectors},
  number       = {3}
}

@article{rasmussen2014spikinga,
  title        = {A Spiking Neural Model Applied to the Study of Human Performance and Cognitive Decline on {{Raven}}'s {{Advanced Progressive Matrices}}},
  author       = {Rasmussen, Daniel and Eliasmith, Chris},
  date         = {2014},
  journaltitle = {Intelligence},
  volume       = {42},
  pages        = {53--82},
  issn         = {0160-2896},
  doi          = {https://doi.org/10.1016/j.intell.2013.10.003},
  url          = {http://www.sciencedirect.com/science/article/pii/S0160289613001542},
  abstract     = {We present a spiking neural model capable of solving a popular test of intelligence, Raven's Advanced Progressive Matrices (RPM). The central features of this model are its ability to dynamically generate the rules needed to solve the RPM and its biologically detailed implementation in spiking neurons. We describe the rule generation processes, and demonstrate the model's ability to use the resulting rules to solve the RPM with similar performance and error patterns to human subjects. Investigating the rules in more detail, we show that they successfully capture abstract patterns in the data, enabling them to generalize to novel matrices. We also show that the same model can be used to solve a separate reasoning task, and demonstrates the expected positive correlation in performance across tasks. Finally, we demonstrate the advantages of the biologically detailed implementation by using the model to connect behavioral and neurophysiological data. Specifically, we investigate two neurophysiological explanations of cognitive decline in aging: neuron loss and representational “dedifferentiation”. We show that manipulations to the model that reflect these neurophysiological hypotheses result in performance changes that match observed human behavioral data.},
  keywords     = {Aging,Cognitive decline,Raven's Progressive Matrices,Spiking neural model,Vector symbolic architectures}
}

@thesis{dewolf2010nocha,
  title       = {{{NOCH}}: {{A}} Framework for Biologically Plausible Models of Neural Motor Control},
  author      = {DeWolf, Travis},
  date        = {2010},
  volume      = {Masters of Mathematics},
  institution = {{University of Waterloo}},
  location    = {{Waterloo, ON}},
  url         = {http://hdl.handle.net/10012/4949},
  abstract    = {This thesis examines the neurobiological components of the motor control system and relates it to current control theory in order to develop a novel framework for models of motor control in the brain. The presented framework is called the Neural Optimal Control Hierarchy (NOCH). A method of accounting for low level system dynamics with a Linear Bellman Controller (LBC) on top of a hierarchy is presented, as well as a dynamic scaling technique for LBCs that drastically reduces the computational power and storage requirements of the system. These contributions to LBC theory allow for low cost, high-precision control of movements in large environments without exceeding the biological constraints of the motor control system.},
  keywords    = {basal ganglia,cerebellum,hierarchy,motor control,motor cortex,NOCH,optimal control},
  type        = {Masters Thesis}
}

@article{eliasmith2012largescale,
  title        = {A Large-Scale Model of the Functioning Brain},
  author       = {Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Yichuan and Rasmussen, Daniel},
  date         = {2012},
  journaltitle = {Science},
  volume       = {338},
  pages        = {1202--1205},
  doi          = {10.1126/science.1225266},
  url          = {http://www.sciencemag.org/cgi/content/full/338/6111/1202?ijkey=y5vph.jw5AgRQ&keytype=ref&siteid=sci}
}

@thesis{choo2018spaun,
  title       = {Spaun 2.0: {{Extending}} the {{World}}’s {{Largest Functional Brain Model}}},
  author      = {Choo, Xuan},
  date        = {2018},
  institution = {{University of Waterloo / UWSpace}},
  url         = {http://hdl.handle.net/10012/13308},
  abstract    = {Building large-scale brain models is one method used by theoretical neuroscientists to understand the way the human brain functions. Researchers typically use either a bottom-up approach, which focuses on the detailed modelling of various biological properties of the brain and places less importance on reproducing functional behaviour, or a top-down approach, which generally aim to reproduce the behaviour observed in real cognitive agents, but typically sacrifices adherence to constraints imposed by the neuro-biology. The focus of this thesis is Spaun, a large-scale brain model constructed using a combination of the bottom-up and top-down approaches to brain modelling. Spaun is currently the world’s largest functional brain model, capable of performing eight distinct cognitive tasks ranging from digit recognition to inductive reasoning. The thesis is organized to discuss three aspects of the Spaun model. First, it describes the original Spaun model, and explores how a top-down approach, known as the Semantic Pointer Architecture (SPA), has been combined with a bottom-up approach, known as the Neural Engineering Framework (NEF), to integrate six existing cognitive models into a unified cognitive model that is Spaun. Next, the thesis identifies some of the concerns with the original Spaun model, and show the modifications made to the network to remedy these issues. It also characterizes how the Spaun model was re-organized and re-implemented (to include the aforementioned modifications) as the Spaun 2.0 model. As part of the discussion of the Spaun 2.0 model, task performance results are presented that compare the original Spaun model and the re-implemented Spaun 2.0 model, demonstrating that the modifications to the Spaun 2.0 model have improved its accuracy on the working memory task, and the two induction tasks. Finally, three extensions to Spaun 2.0 are presented. These extensions take advantage of the re-organized Spaun model, giving Spaun 2.0 new capabilities – a motor system capable of adapting to unknown force fields applied to its arm; a visual system capable of processing 256×256 full-colour images; and the ability to follow general instructions. The Spaun model and architecture presented in this thesis demonstrate that by using the SPA and the NEF, it is not only possible to construct functional large-scale brain models, but to do so in a manner that supports complex extensions to the model. The final Spaun 2.0 model consists of approximately 6.6 million neurons, can perform 12 cognitive tasks, and has been demonstrated to reproduce behavioural and neurological data observed in natural cognitive agents.},
  type        = {PhD thesis}
}


@article{jahnke1968delayed,
  title        = {Delayed Recall and the Serial-Position Effect of Short-Term Memory.},
  author       = {Jahnke, John C.},
  date         = {1968},
  journaltitle = {Journal of Experimental Psychology},
  volume       = {76},
  pages        = {618--622},
  publisher    = {{American Psychological Association}},
  issn         = {0022-1015(Print)},
  doi          = {10.1037/h0025692},
  file         = {/home/andreas/Uni/PhD/bibliography/pdfs/Jahnke - 1968 - Delayed recall and the serial-position effect of s.pdf},
  issue        = {4, Pt.1},
  keywords     = {*Interstimulus Interval,*Recall (Learning),*Serial Learning,*Stimulus Intervals,*Words (Phonetic Units),Intertrial Interval,Retention},
  place        = {US}
}

@misc{mikolov2013efficient,
  author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  title  = {Efficient Estimation of Word Representations in Vector Space},
  year   = {2013},
  eprint = {arXiv:1301.3781}
}

@article{strata1999dale,
  title        = {Dale’s Principle},
  author       = {Strata, Piergiorgio and Harvey, Robin},
  date         = {1999},
  journaltitle = {Brain Research Bulletin},
  volume       = {50},
  pages        = {349--350},
  issn         = {0361-9230},
  doi          = {https://doi.org/10.1016/S0361-9230(99)00100-8},
  url          = {http://www.sciencedirect.com/science/article/pii/S0361923099001008},
  file         = {/home/astoecke/Uni/PhD/bibliography/pdfs/Strata and Harvey - 1999 - Dale’s principle.pdf},
  number       = {5}
}

@article{stockel2019passive,
  title        = {Passive Nonlinear Dendritic Interactions as a General Computational Resource in Functional Spiking Neural Networks},
  author       = {Stöckel, Andreas and Eliasmith, Chris},
  date         = {2019},
  journaltitle = {arXiv},
  eprint       = {arXiv:1904.11713}
}


@article{marr1969theory,
  title        = {A Theory of Cerebellar Cortex},
  author       = {Marr, D},
  date         = {1969-06},
  journaltitle = {The Journal of physiology},
  shortjournal = {J Physiol},
  volume       = {202},
  pages        = {437--470},
  issn         = {0022-3751},
  doi          = {10.1113/jphysiol.1969.sp008820},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/5784296},
  abstract     = {1. A detailed theory of cerebellar cortex is proposed whose consequence is that the cerebellum learns to perform motor skills. Two forms of input-output relation are described, both consistent with the cortical theory. One is suitable for learning movements (actions), and the other for learning to maintain posture and balance (maintenance reflexes).2. It is known that the cells of the inferior olive and the cerebellar Purkinje cells have a special one-to-one relationship induced by the climbing fibre input. For learning actions, it is assumed that:(a) each olivary cell responds to a cerebral instruction for an elemental movement. Any action has a defining representation in terms of elemental movements, and this representation has a neural expression as a sequence of firing patterns in the inferior olive; and(b) in the correct state of the nervous system, a Purkinje cell can initiate the elemental movement to which its corresponding olivary cell responds.3. Whenever an olivary cell fires, it sends an impulse (via the climbing fibre input) to its corresponding Purkinje cell. This Purkinje cell is also exposed (via the mossy fibre input) to information about the context in which its olivary cell fired; and it is shown how, during rehearsal of an action, each Purkinje cell can learn to recognize such contexts. Later, when the action has been learnt, occurrence of the context alone is enough to fire the Purkinje cell, which then causes the next elemental movement. The action thus progresses as it did during rehearsal.4. It is shown that an interpretation of cerebellar cortex as a structure which allows each Purkinje cell to learn a number of contexts is consistent both with the distributions of the various types of cell, and with their known excitatory or inhibitory natures. It is demonstrated that the mossy fibre-granule cell arrangement provides the required pattern discrimination capability.5. The following predictions are made.(a) The synapses from parallel fibres to Purkinje cells are facilitated by the conjunction of presynaptic and climbing fibre (or post-synaptic) activity.(b) No other cerebellar synapses are modifiable.(c) Golgi cells are driven by the greater of the inputs from their upper and lower dendritic fields.6. For learning maintenance reflexes, 2(a) and 2(b) are replaced by2'. Each olivary cell is stimulated by one or more receptors, all of whose activities are usually reduced by the results of stimulating the corresponding Purkinje cell.7. It is shown that if (2') is satisfied, the circuit receptor --{$>$} olivary cell --{$>$} Purkinje cell --{$>$} effector may be regarded as a stabilizing reflex circuit which is activated by learned mossy fibre inputs. This type of reflex has been called a learned conditional reflex, and it is shown how such reflexes can solve problems of maintaining posture and balance.8. 5(a), and either (2) or (2') are essential to the theory: 5(b) and 5(c) are not absolutely essential, and parts of the theory could survive the disproof of either.},
  eprint       = {5784296},
  eprinttype   = {pubmed},
  file         = {/home/andreas/Uni/PhD/bibliography/pdfs/Marr - 1969 - A theory of cerebellar cortex.pdf},
  keywords     = {Animals,Axons/physiology,Cats,Cerebellar Cortex/*physiology,Dendrites/physiology,Humans,Learning,Motor Skills,Olivary Nucleus/physiology,Postural Balance,Posture,Purkinje Cells/cytology/physiology,Reflex,Sensory Receptor Cells/physiology,Stellate Ganglion/physiology,Synapses/physiology},
  langid       = {english},
  number       = {2}
}

@incollection{ito2010cerebellar,
  title     = {Cerebellar {{Cortex}}},
  booktitle = {Handbook of {{Brain Microcircuits}}},
  author    = {Ito, Masao},
  editor    = {Shepherd, Gordon and Grillner, Sten},
  date      = {2010-08},
  edition   = {1st},
  pages     = {293--300},
  publisher = {{Oxford University Press}},
  location  = {{Oxford, UK}},
  url       = {https://oxfordmedicine.com/view/10.1093/med/9780195389883.001.0001/med-9780195389883-chapter-028},
  isbn      = {978-0-19-538988-3}
}


@incollection{llinas2010olivocerebellar,
  title     = {Olivocerebellar {{System}}},
  booktitle = {Handbook of {{Brain Microcircuits}}},
  author    = {Llinás, Rodolfo R.},
  editor    = {Shepherd, Gordon and Grillner, Sten},
  date      = {2010-08},
  edition   = {1st},
  pages     = {301--308},
  publisher = {{Oxford University Press}},
  location  = {{Oxford, UK}},
  url       = {https://oxfordmedicine.com/view/10.1093/med/9780195389883.001.0001/med-9780195389883-chapter-029},
  isbn      = {978-0-19-538988-3}
}

@article{london2005dendritic,
  title        = {Dendritic {{Computation}}},
  author       = {London, Michael and Häusser, Michael},
  date         = {2005},
  journaltitle = {Annual Review of Neuroscience},
  volume       = {28},
  pages        = {503--532},
  doi          = {10.1146/annurev.neuro.28.061604.135703},
  url          = {https://doi.org/10.1146/annurev.neuro.28.061604.135703},
  abstract     = {One of the central questions in neuroscience is how particular tasks, or computations, are implemented by neural networks to generate behavior. The prevailing view has been that information processing in neural networks results primarily from the properties of synapses and the connectivity of neurons within the network, with the intrinsic excitability of single neurons playing a lesser role. As a consequence, the contribution of single neurons to computation in the brain has long been underestimated. Here we review recent work showing that neuronal dendrites exhibit a range of linear and nonlinear mechanisms that allow them to implement elementary computations. We discuss why these dendritic properties may be essential for the computations performed by the neuron and the network and provide theoretical and experimental examples to support this view.},
  eprint       = {16033324},
  eprinttype   = {pmid},
  number       = {1}
}

@report{voelker2017efficiently,
  title       = {Efficiently Sampling Vectors and Coordinates from the N-Sphere and n-Ball},
  author      = {Voelker, Aaron R. and Gosmann, Jan and Stewart, Terrence C.},
  date        = {2017-01},
  institution = {{Centre for Theoretical Neuroscience}},
  location    = {{Waterloo, ON}},
  issn        = {CTN-TR-20170104-01},
  doi         = {10.13140/RG.2.2.15829.01767/1},
  url         = {https://www.researchgate.net/publication/312056739_Efficiently_sampling_vectors_and_coordinates_from_the_n-sphere_and_n-ball},
  abstract    = {We provide a short proof that the uniform distribution of points for the n-ball is equivalent to the uniform distribution of points for the (n + 1)-sphere projected onto n dimensions. This implies the surprising result that one may uniformly sample the n-ball by instead uniformly sampling the (n + 1)-sphere and then arbitrarily discarding two coordinates. Consequently, any procedure for sampling coordinates from the uniform (n + 1)-sphere may be used to sample coordinates from the uniform n-ball without any modification. For purposes of the Semantic Pointer Architecture (SPA), these insights yield an efficient and novel procedure for sampling the dot-product of vectors—sampled from the uniform ball—with unit-length encoding vectors.}
}
