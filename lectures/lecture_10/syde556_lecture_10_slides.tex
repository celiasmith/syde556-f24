% !TeX spellcheck = en_GB
\ifcsname SlidesDistr\endcsname%
\documentclass[handout,aspectratio=169]{beamer}
\else%
\documentclass[aspectratio=169]{beamer}
\fi%
\input{../syde556_lecture_slides_preamble}

\usepackage{ragged2e}

\newcommand{\Pred}[1]{\mathbf{\textcolor{scarletred3}{#1}}}
\newcommand{\Obj}[1]{\mathtt{\textcolor{skyblue3}{#1}}}
\newcommand{\Fun}[1]{\mathit{\textcolor{chameleon3}{#1}}}
\newcommand{\CC}{\circledast}

\date{November 6 \& 11, 2024}
\title{SYDE 556/750 \\ Simulating Neurobiological Systems \\ Lecture 10: Symbols and Symbol-like Representations}

\begin{document}
	
	\begin{frame}{}
		\vspace{0.5cm}
		\begin{columns}[c]
			\column{0.6\textwidth}
			\MakeTitle
			\column{0.4\textwidth}
			\includegraphics[width=\textwidth]{media/bell_telephone_magazine_1922_14733423296_small_2.jpg}
		\end{columns}
	\end{frame}

	\begin{frame}{Classical Representation of Knowledge}
		\begin{itemize}
			\item \enquote{The number nine comes after the number eight}: $$\Pred{isSucc}(\Obj{NINE}, \Obj{EIGHT})\,.$$
			\item \enquote{All dogs chase cats}: $$\forall x \forall y \, \big ( \Pred{isDog}(x) \wedge \Pred{isCat}(y) \big) \rightarrow \Pred{doesChase}(x, y)\,.$$
			\item \enquote{Anne knows that Bill thinks that Charlie likes Dave}:$$\Pred{knows}\Big(\Obj{ANNE}, ``\Pred{thinks}\big(\Obj{BILL}, `\Pred{likes}(\Obj{CHARLIE}, \Obj{DAVE})\text{'}\big)\text{''}\Big)\,.$$
		\end{itemize}
	\end{frame}

	\begin{frame}{Jackendoff's Challenges}
		\begin{itemize}
			\item The Binding Problem
			\item The Problem of Two
			\item The Problem of Variables
			\item Working Memory versus Long-Term Memory
		\end{itemize}
	\end{frame}

	\begin{frame}{Solution Attempt 1: Neural Synchrony (I)}
		\includegraphics[width=\textwidth]{media/eliasmith_2013_lisa.pdf}
	\end{frame}

	\begin{frame}{Solution Attempt 1: Neural Synchrony (II)}
		\begin{multicols}{2}
			\begin{itemize}
				\setlength{\itemsep}{0.33cm}
				\item[\OPlus] Solves the binding problem
				\item[\OMeh] Localist representation
				\item[\OMeh] Unclear how to solve problems 2 to 4
				\columnbreak
				\item[\OMinus] Unclear how these oscillations are generated and controlled
				\item[\OMinus] Unclear how the representations are processed
				\item[\OMinus] Exponential explosion of neurons required to represent concepts
			\end{itemize}
		\end{multicols}
	\end{frame}

	\begin{frame}{Solution Attempt 2: Neural Blackboard Architecture (I)}
		\centering
		\includegraphics[height=7.25cm]{media/eliasmith_2013_blackboard.pdf}
	\end{frame}

	\begin{frame}{Solution Attempt 2: Neural Blackboard Architecture (II)}
		\centering
		\begin{multicols}{2}
			\begin{itemize}
				\setlength{\itemsep}{0.33cm}
				\item[\OPlus] Fewer resources than LISA
				\item[\OPlus] Solves all four of Jackendoffs challenges (according to the authors)
				\item[\OPlus] Explains limitations of human sentence representation
				\item[\OMeh] (At least partially) localist representation
				\columnbreak
				\item[\OMinus] Specific neural structure; does not match biology
				\item[\OMinus] Large number of neurons; about $500\times 10^6$ to represent simple sentences
				\item[\OMinus] Only considers \emph{representation},\\no control structures
			\end{itemize}
		\end{multicols}
	\end{frame}

	\begin{frame}{Solution Attempt 3: Vector Operators (I)}
		\centering
		\textbf{Idea:}  High-dimensional vectors $\vec x \in \mathbb{R}^d$ represent symbols; bind using tensor product\\\vspace*{-0.33cm}
		\small
		\begin{align*}
		\begin{pmatrix}a_1\\a_2\\a_3\end{pmatrix} \otimes \begin{pmatrix}b_1\\b_2\\b_3\end{pmatrix} &=
		\begin{pmatrix}
		a_1 b_1 & a_1 b_2 & a_1 b_3 \\
		a_2 b_1 & a_2 b_2 & a_2 b_3 \\
		a_3 b_1 & a_3 b_2 & a_3 b_3
		\end{pmatrix} && \text{(Outer product)} \\[0.25cm]
		\begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \otimes
		\begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} &=
		\begin{pmatrix}
		a_{11} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} &
		a_{12} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} \\
		a_{21} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} &
		a_{22} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix}
		\end{pmatrix} && \text{(Tensor product)}\\ &= 
		\begin{pmatrix}
		a_{11} b_{11} & a_{11} b_{12} & a_{12} b_{11} & a_{12} b_{12} \\
		a_{11} b_{21} & a_{11} b_{22} & a_{12} b_{21} & a_{12} b_{22} \\
		a_{21} b_{11} & a_{21} b_{12} & a_{22} b_{11} & a_{22} b_{12} \\
		a_{21} b_{21} & a_{21} b_{22} & a_{22} b_{21} & a_{22} b_{22}
		\end{pmatrix}
		\end{align*}
	\end{frame}

  \begin{frame}{Solution Attempt 3: Vector Operators (II)}
		\centering
			\begin{itemize}
				\setlength{\itemsep}{0.33cm}
				\item[\OPlus] Solves the binding problem, the problem of two, and the problem of variables
				\item[\OMeh] Unclear how to solve the working vs long-term memory problem
        \item[\OMinus] Scales extremely poorly $d^n$ for $n$ binding operations
			\end{itemize}
	\end{frame}

	\begin{frame}{A Deeper Problem: Cognitive Science vs. Neuroscience}
		\begin{columns}
		\column{0.63\textwidth}
		\begin{itemize}
			\setlength{\itemsep}{0.75cm}
			\item Trying very hard to map purely symbolic architectures onto neurons.
			\item Neural aspects are treated as\\\emph{mere implementation details}.
			\item Instance of \hl{top-down modelling}:\\High-level cognitive architectures are mapped onto biology.
			\item Hope of many cognitive scientists:\\If successful, \hl{neurons do not matter}.
		\end{itemize}
		\column{0.37\textwidth}
		\includegraphics[width=\textwidth]{media/levels_direction_top_down.pdf}
		\end{columns}
	\end{frame}

	\begin{frame}{Vector Symbolic Algebras}
    VSAs identify four key algebraic operations for capturing symbol-like representations:
		\begin{columns}\column{0.6\textwidth}
		\begin{enumerate}[1.]
			\setlength{\itemsep}{0.5cm}
			\item Binding
			\begin{align*}
	 			\CC &: \mathbb{R}^d \times \mathbb{R}^d \longrightarrow \mathbb{R}^d
	 		\end{align*}
			\item Bundling
			\begin{align*}
        + &: \mathbb{R}^d \times \mathbb{R}^d \longrightarrow \mathbb{R}^d
			\end{align*}
			\item Permutation: We won't worry about this one.
      \item Similarity
			\begin{align*}
        sim(x,y) &: \mathbb{R}^d \times \mathbb{R}^d \longrightarrow \mathbb{R}^1
			\end{align*}

		\end{enumerate}\end{columns}
	\end{frame}
  
  \begin{frame}{Binding Operator Properties for Vector Symbolic Algebras}
		\begin{columns}\column{0.6\textwidth}
		\begin{enumerate}[i.]
			\setlength{\itemsep}{0.75cm}
			\item Preservation of Dimensionality
			\begin{align*}
	 			\CC &: \mathbb{R}^d \times \mathbb{R}^d \longrightarrow \mathbb{R}^d
	 		\end{align*}
			\item Approximately Reversible
			\begin{align*}
				\vec x &\approx (\vec x \CC \vec y) \CC \vec y^{-1}
			\end{align*}
			\item Dissimilar to Inputs
			\begin{align*}
				0 &\approx \langle \vec x \CC \vec y, \vec x \rangle \,, 0 \approx \langle \vec x \CC \vec y, \vec y \rangle
			\end{align*}
		\end{enumerate}\end{columns}
	\end{frame}

	\begin{frame}{Sentence Encoding Revisited}
		\begin{itemize}
			\item \enquote{The number nine comes after the number eight}:
			\begin{align*}
			\Obj{NUMBER} \CC \Obj{NINE} + \Obj{SUCC} \CC \Obj{EIGHT} \,.
			\end{align*}
			\item \enquote{The dog chases the cat}:
			\begin{align*}
			\Obj{DOG} \CC \Obj{SUBJ} + \Obj{CAT} \CC \Obj{OBJ} + \Obj{CHASE} \CC \Obj{VERB} \,.
			\end{align*}
			\item \enquote{Anne knows that Bill thinks that Charlie likes Dave}:
			\begin{align*}
			\Obj{SUBJ} \CC \Obj{ANNE} + \Obj{ACT} \CC \Obj{KNOWS} + \Obj{OBJ} \CC \\\Big(\Obj{SUBJ} \CC \Obj{BILL} + \Obj{ACT} \CC \Obj{THINKS} + \Obj{OBJ} \CC \\ \big(\Obj{SUBJ} \CC \Obj{CHARLIE} + \Obj{ACT} \CC \Obj{LIKES} + \Obj{OBJ} \CC \Obj{DAVE}\big)\Big) \,.
			\end{align*}
		\end{itemize}
		\vspace{-0.25cm}
		\begin{itemize}
			\centering
			\item<2->[\symbolfont ⚠] \hl{Compression of information; graceful degradation; depends on $d$}
		\end{itemize}
	\end{frame}

	\begin{frame}{Using the Reversibility Property to Answer Questions}
		\begin{itemize}
		\item \enquote{A blue square and a red circle:}
		\begin{align*}
			\vec x = \Obj{BLUE} \CC \Obj{SQUARE} + \Obj{RED} \CC \Obj{CIRCLE} \,.
		\end{align*}\\[0.5cm]
		\item \enquote{Which object is blue?}
		\begin{align*}
			\vec y &= \big( \Obj{BLUE} \CC \Obj{SQUARE} + \Obj{RED} \CC \Obj{CIRCLE} \big) \CC \Obj{BLUE}^{-1} \\
			&= \big( \Obj{BLUE} \CC \Obj{SQUARE} \big) \CC \Obj{BLUE}^{-1} + \big( \Obj{RED} \CC \Obj{CIRCLE} \big) \CC \Obj{BLUE}^{-1} \\
			&\approx \Obj{SQUARE} + \underbrace{\Obj{RED} \CC \Obj{CIRCLE} \CC \Obj{BLUE}^{-1}}_{\text{\enquote{noise}}} \\
			&\approx \Obj{SQUARE} \,.
		\end{align*}
		\end{itemize}
		\vspace{-0.25cm}
		\begin{itemize}
			\centering
			\item<2->[\symbolfont ⚠]\hl{Supposes that there is a set of valid symbols $\Rightarrow$ \enquote{Cleanup Memory}}
		\end{itemize}
	\end{frame}

	\begin{frame}{VSAs: Potential Binding Operators (I)}
		\begin{align*}
		\begin{pmatrix}1 \\ 0 \\ 1 \\ 0\end{pmatrix} \oplus \begin{pmatrix}1 \\ 1 \\ 0 \\ 0\end{pmatrix} &= \begin{pmatrix}0 \\ 1 \\ 1 \\ 0\end{pmatrix} && \textit{(XOR)}\\
		\begin{pmatrix}A \\ B \\ C \\ D\end{pmatrix} \odot \begin{pmatrix}E \\ F \\ G \\ H\end{pmatrix} &= \begin{pmatrix}AE \\ BF \\ CG \\ DH\end{pmatrix} && \textit{(Hadamard Product)}
		\end{align*}
	\end{frame}

	\begin{frame}{VSAs: Potential Binding Operators (II)}
		\begin{align*}
		\begin{pmatrix}A \\ B \\ C \\ D\end{pmatrix} \CC \begin{pmatrix}E \\ F \\ G \\ H\end{pmatrix} &= \begin{pmatrix}AE &+& BH &+& CG &+& DF \\ AF &+& BE &+& CH &+& DG \\ AG &+& BF &+& CE &+& DH \\ AH &+& BG &+& CF &+& DE\end{pmatrix} && \textit{(Circular Convolution)}
		\end{align*}
		\vspace{0.25cm}
		Circular Convolution is a \enquote{compressed} outer product:\\[0.25cm]
		\hspace{0.7cm}\includegraphics{media/cconv_outer_product.pdf}
	\end{frame}

	\begin{frame}{Circular Convolution: Dissimilarity and Reversibility}
		\centering
		\includegraphics[height=7.25cm]{media/cconv_sim.pdf}
	\end{frame}

  \begin{frame}{Circular Convolution: Encoding Numbers}
		\begin{itemize}
		\item Spaun uses an interesting encoding to capture number relations.
		\item Start with a random vector $\Obj{ONE}$, then
    \begin{align*}
      \Obj{TWO} &= \Obj{ONE} \CC \Obj{ONE} \,, \\
      \Obj{THREE} &= \Obj{ONE} \CC \Obj{TWO} = \Obj{ONE} \CC \Obj{ONE} \CC \Obj{ONE} \,, \\
      \Obj{NUMBER\text{-$k$}} &= \underbrace{\Obj{ONE} \CC \Obj{ONE} \CC \text{\textellipsis} \CC \Obj{ONE}}_{\text{$k$-times}}\\
        &=\Obj{ONE}^k \,.
    \end{align*}
		\item Which is a slight abuse of notation. Also note,
    \begin{align*}
      \Obj{ONE}^k &= \mathcal{DFT}^{-1} \big( \mathcal{DFT}(\Obj{ONE})^k \big) \,,
    \end{align*}
    \item Where inside the bracket is a Hadamard exponent.
		\end{itemize}
	\end{frame}

	\begin{frame}{Raven's Progressive Matrices (I)}
		\begin{columns}
			\column{0.33\textwidth}%
			\centering%
			\includegraphics<1->[width=0.875\textwidth]{media/ravens_example_c.pdf}%
			\column{0.33\textwidth}%
			\centering%
			\includegraphics<2->[width=0.875\textwidth]{media/ravens_example_a.pdf}%
			\column{0.33\textwidth}%
			\centering%
			\includegraphics<3->[width=0.875\textwidth]{media/ravens_example_b.pdf}%
		\end{columns}
	\end{frame}

	\videoframe{spaun_remembering_list}{U_Q6Xjz9QHg}

	\videoframe{spaun_completing_pattern}{Q_LRvnwnYp8}

	\begin{frame}{Raven's Progressive Matrices (II)}
		\begin{columns}
			\column{0.33\textwidth}%
			\centering%
			\includegraphics<1->[width=0.875\textwidth]{media/ravens_example_c.pdf}%
			\column{0.66\textwidth}%
			Representing cells:
			\begin{align*}
				\Obj{C1} &= \Obj{ONE}  \CC \Obj{P1}                                                       \,, \\
				\Obj{C2} &= \Obj{ONE}  \CC \Obj{P1} + \Obj{ONE}  \CC \Obj{P2}                             \,, \\
				\Obj{C3} &= \Obj{ONE}  \CC \Obj{P1} + \Obj{ONE}  \CC \Obj{P2} + \Obj{ONE}  \CC \Obj{P3}   \,, \\
				\Obj{C4} &= \Obj{FOUR} \,, \\
				\Obj{C5} &= \Obj{FOUR} \CC \Obj{P1} + \Obj{FOUR} \CC \Obj{P2}                             \,, \\
				\Obj{C6} &= \Obj{FOUR} \CC \Obj{P1} + \Obj{FOUR} \CC \Obj{P2} + \Obj{FOUR} \CC \Obj{P3}   \,, \\
				\Obj{C7} &= \Obj{FIVE} \CC \Obj{P1}                                                       \,, \\
				\Obj{C8} &= \Obj{FIVE} \CC \Obj{P1} + \Obj{FIVE} \CC \Obj{P2}                             \,.
			\end{align*}
		\end{columns}
	\end{frame}

	\begin{frame}{Raven's Progressive Matrices (III)}
		\begin{columns}
			\column{0.33\textwidth}%
			\centering%
			\includegraphics<1->[width=0.875\textwidth]{media/ravens_example_c.pdf}%
			\column{0.66\textwidth}%
			Extracting the horizontal rule:
			\begin{align*}
				\Obj{T1} &= \Obj{C2} \CC \Obj{C1}^{-1} \,, &
				\Obj{T4} &= \Obj{C6} \CC \Obj{C5}^{-1} \,, \\
				\Obj{T2} &= \Obj{C3} \CC \Obj{C2}^{-1} \,, &
				\Obj{T5} &= \Obj{C8} \CC \Obj{C7}^{-1} \,, \\
				\Obj{T3} &= \Obj{C5} \CC \Obj{C4}^{-1} \,.
			\end{align*}
			\vspace{-0.25cm}
			\begin{align*}
				\Obj{T} &= \frac{\Obj{T1} + \Obj{T2} + \Obj{T3} + \Obj{T4} + \Obj{T5}}5 \,.
			\end{align*}
			Making a prediction:
			\begin{align*}
				\Obj{C9} &= \Obj{C8} \CC \Obj{T} \\ &\approx \Obj{FIVE}  \CC \Obj{P1} + \Obj{FIVE}  \CC \Obj{P2} + \Obj{FIVE}  \CC \Obj{P3} \,.
			\end{align*}
		\end{columns}
	\end{frame}
	
  \begin{frame}{Jackendoff's Challenges with a VSA}
		\begin{itemize}
			\item The Binding Problem
			\item The Problem of Two
			\item The Problem of Variables
			\item Working Memory versus Long-Term Memory
		\end{itemize}
	\end{frame}

	\backupbegin

	\begin{frame}[noframenumbering]{Image sources}
		\small
		\textbf{Title slide}\\Bell telephone magazine, 1922,  American Telephone and Telegraph Company\\ \href{https://commons.wikimedia.org/wiki/File:Bell_telephone_magazine_(1922)_(14733423296).jpg}{Wikimedia}.
	\end{frame}

	\backupend
	
\end{document}
