% !TeX spellcheck = en_GB
\documentclass[10pt,letterpaper,oneside]{article}
\input{../syde556_lecture_notes_preamble}

\date{Nov 6 \& 11, 20224}
\title{SYDE 556/750 \\ Simulating Neurobiological Systems \\ Lecture 10: Symbols and Symbol-like Representations}


\begin{document}

\MakeTitle{\textbf{Accompanying Readings: Chapter 3, 4 and 9 of \enquote{How to Build a Brain}}}

\section{Introduction}

\Note{So far we have mostly been concerned with representing vectors $\vec x$ in populations of neurons. In this lecture we instead think about the representation of \emph{symbols}, leading up to the Semantic Pointer Architecture (SPA) which we will discuss in the next lecture.}

As discussed at the beginning of the course, our ultimate goal is to build models of human brains, and, by extension, human cognition. So far, it might seem as if we had not made much progress in this direction. What we have discussed up to this point is \enquote{merely} allowing us to represent vectors in a population of neurons, to compute transformations of those represented values within the connections between populations, and to build dynamical systems treating the represented values as control-theoretic state variables.

In particular, one hallmark of human cognition is \emph{language}, and it is not clear how to apply the NEF to language, or facts expressed in language. Examples of language-based statements that we may want to encode are sentences such as \enquote{the number eight comes after the number nine}, \enquote{all dogs chase cats}, or \enquote{Anne knows that Bill thinks that Charlie likes Dave}.

We attempt to solve this problem by first having a look at other modelling approaches in Cognitive Neuroscience. In particular, we will have a look at so called Vector Symbolic Algebras (VSAs), which can be easily mapped onto the NEF. In the next lecture we then use VSAs to build a particular model of cognition, the Semantic Pointer Architecture (SPA).

\section{Neural Theories of Cognition}

\Note{For more details on the theories of cognition listed here, have a look at Chapter 9 of \enquote{How to Build a Brain}~\cite{eliasmith2013how}.}

Traditionally, both cognitive scientists and early computer scientists working on artificial intelligence have constructed theories of cognition that involve processing structured information using symbol-based representational frameworks, such as predicate logic. For example, the statements mentioned above could be written like this (this notation is supposed to resemble predicate logic):
\newcommand{\Pred}[1]{\mathbf{\textcolor{Crimson}{#1}}}
\newcommand{\Obj}[1]{\mathtt{\textcolor{RoyalBlue}{#1}}}
\newcommand{\Fun}[1]{\mathit{\textcolor{ForestGreen}{#1}}}
\begin{itemize}
	\item \enquote{The number nine comes after the number eight}: $\Pred{isSucc}(\Obj{NINE}, \Obj{EIGHT})\,$.
	\item \enquote{All dogs chase cats}: $\forall x \forall y \, \big ( \Pred{isDog}(x) \wedge \Pred{isCat}(y) \big) \rightarrow \Pred{doesChase}(x, y)\,$.
	\item \enquote{Anne knows that Bill thinks that Charlie likes Dave}:$$\Pred{knows}\Big(\Obj{ANNE}, ``\Pred{thinks}\big(\Obj{BILL}, `\Pred{likes}(\Obj{CHARLIE}, \Obj{DAVE})\text{'}\big)\text{''}\Big)\,.$$
\end{itemize}
Both computer scientists and cognitive scientists have built cognitive models that are roughly based on representations akin to the ones listed above. These \enquote{symbolic} approaches are generally quite successful in modelling certain aspects of human cognition and can be made to fit behavioural data. However, they do not answer the question of how these symbols are represented and manipulated within a human brain.

\subsection{Jackendoff's Challenges for Cognitive Neuroscience}

In his 2002 book \enquote{Foundations of Language: Brain, Meaning, Grammar, Evolution} \cite{jackendoff2002foundations}, linguist Ray Jackendoff poses four challenges aimed at cognitive neuroscientists who aim to build a neural model of language. These challenges are
\newcommand{\RS}{$\color{Crimson}\blacksquare$}
\newcommand{\BC}{$\color{RoyalBlue}\bullet$}
\begin{itemize}
	\item \textbf{The Binding Problem.} Suppose you see a red square and a blue circle. How does the concept of \enquote{red} get bound with the concept of \enquote{square}, and how is it kept separate from \enquote{blue} and \enquote{circle}?
	\item \textbf{The Problem of Two.} Consider the sentence \enquote{the little star is besides the big star}. How do we keep those two uses of the concept \enquote{star} separate? For example, if there was a group of neurons representing the concept of \enquote{star}, how can this group of neurons both represent the \enquote{little star}, the \enquote{big star}, as well as the fact that the two stars are next to each other?
	\item \textbf{The Problem of Variables.} Grammar imposes certain rules on sentences. For example, it is \enquote{correct} to say \enquote{blue $x$}, if $x$ is a noun, but not \enquote{blue $y$}, if $y$ is a verb. Correspondingly, the question is how these rules, which rely on placeholders, or \enquote{variables}, are represented in the brain.
	\item \textbf{Working Memory versus Long-Term Memory.} We can both \emph{use} sentences (and keep them in working memory; i.e., current neural activities), while also being able to \emph{store} them for very long times (long-term memory; synaptic weights). A neural architecture of language must explain how sentences can be transferred from working memory to long-term memory and back. In other words, we must be able to turn representations from neural activities into synaptic weight changes.
\end{itemize}

\subsection{Solution Attempt 1: Neural Synchrony (Oscillations)}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{media/eliasmith_2013_lisa.pdf}
	\caption{The LISA architecture. Boxes corresponds to groups of neurons representing individual concepts, lines to synaptic conncetions. Concepts are \enquote{bound} by neurons oscillating at the same frequency/phase. Figure copied from Figure~9.1~in Eliasmith, 2013 \cite{eliasmith2013how}, which is in turn adapted from Hummel and Holyoak, 2003~\cite{hummel2003symbolicconnectionist}.}
	\label{fig:eliasmith_2013_lisa.pdf}
\end{figure}

One relatively early theory of how the \enquote{binding problem} could be solved is the LISA architecture \cite{hummel2003symbolicconnectionist}, which was developed in the early 1990s. This architecture solves the binding problem by proposing to exploit \enquote{neural synchrony}.

A central idea of this approach is that individual groups of neurons represent concepts. This is a so called \emph{localist} representation; individual spatially colocated groups of neurons correspond to individual concepts. Higher-level concepts can be created by connecting groups of neurons. For example, the neurons representing the symbol \enquote{cat} can be connected to neurons representing the concepts \enquote{furry}, \enquote{solitary}, and \enquote{animal} (cf.~\cref{fig:eliasmith_2013_lisa.pdf}).

Concepts are bound by neurons representing the concept oscillating at the same frequency. That is, for our example of \enquote{red square and blue circle}, the neuron populations of these concepts would oscillate synchronously in phase, while separate, currently active concepts oscillate with a different frequency and/or out of phase.

\newpage

Let's evaluate this architecture.
\begin{itemize}
	\item[\OPlus] The architecture solves the binding problem.
	\item[\OMeh] The architecture assumes localist representation, i.e., a few neurons represent a single concept. Most evidence points towards a distributed representation.
	\item[\OMeh] It is unclear how this architecture would solve the \enquote{problem of two} -- maybe the same symbol used multiple times could oscillate with two superimposed frequencies.
	\item[\OMeh] This architecture cannot solve Jackendoff's third and fourth challenge.
	\item[\OMinus] It is unclear how these oscillations are generated and controlled in the first place, i.e., how is language translated into activation of these localist representations?
	\item[\OMinus] Furthermore, it us unclear how the representations are processed---which mechanism is reading out the oscillations and decides which neural ensembles are active together?
	\item[\OMinus] Finally, we have an exponential explosion of neurons required to represent all kinds of different concepts. For example, assume that there are $10^4$ symbols. Then, we have on the order of $10^8$ possible second-order combinations of symbols (\enquote{subpropositions}), and on the order of $10^{16}$ possible propositions. This already exceeds the number of neurons in the brain by a factor of one to ten thousand.
\end{itemize}

\subsection{Solution Attempt 2: Neural Blackboard Architecture}

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{media/eliasmith_2013_blackboard.pdf}
	\caption{Neural Blackboard Architecture. \textbf{(a)} Groups of neurons represent concepts (words)---they are connected via a switchboard circuit (symbolised by $\circledast$ and depicted in panels \textbf{(b)} and \textbf{(c)}) to slots corresponding to individual roles in a sentence. These roles are then combined via switchboards into higher-level sentences. Image copied from Figure~9.2~in Eliasmith, 2013~\cite{eliasmith2013how}, which is in term adapted from van der Velde and de Kamps, 2006~\cite{vandervelde2006neural}.}
\end{figure}

An attempt at solving all four problems posed by Jackendoff is the \enquote{Neural Blackboard Architecture} by van der Velde and de Kamps~\cite{vandervelde2006neural}. The idea is to solve the exponential growth problem posed by synchrony approaches by introducing switchboard circuits that can arbitrarily route signals from neurons representing individual concepts to so called \enquote{assemblies} representing bound concepts. These high level concepts can then be associated with specific roles a concept might have in a sentence.
\begin{itemize}
	\item[\OPlus] This approach has a much lower resource consumption than LISA.
	\item[\OPlus] Can solve all four of Jackendoffs challenges (according to the authors).
	\item[\OPlus] Explains limitations of human sentence representation.
	\item[\OMeh] This is still a (at least partially) localist representation.
	\item[\OMinus] Uses a very particular structure that does not really seem to match biology.
	\item[\OMinus] Uses a very large number of neurons; about $500\times 10^6$ to represent simple sentences (those constructed out of 2,000 verbs and 4,000 nouns). For average adult vocabulary sizes of about 60,000 words, this approach requires more neurons than are in all language areas combined.
	\item[\OMinus] Only considers sentence representation, but not how they can be transformed and manipulated.
\end{itemize}

\subsection{Solution Attempt 3: Vector Operators}

As mentioned, both approaches discussed above are \emph{localist}. We could instead try to decouple concepts from the underlying neural substrate. That is, populations of neurons can represent different symbols, and \enquote{send} represented symbols for processing to other groups of neurons.

This would be more in line with the kind of information processing suggested by the NEF. Knowing that we can represent vectors in neural populations, we could represent symbols as vectors $\vec x \in \mathbb{R}^d$. For now, let's assume that these vectors $\vec x$ are randomly generated.

This idea is actually quite old, and there have been multiple suggestions as for how to symbol vectors $\vec x$ and $\vec y$ could be bound together. One approach suggested by Smolensky in 1990 \cite{smolensky1990tensor} is to simply use a tensor product $\vec x \otimes \vec y$, a generalisation of a vector outer product to matrices:
\begin{align*}
	\begin{pmatrix}a_1\\a_2\\a_3\end{pmatrix} \otimes \begin{pmatrix}b_1\\b_2\\b_3\end{pmatrix} &=
	\begin{pmatrix}
		a_1 b_1 & a_1 b_2 & a_1 b_3 \\
		a_2 b_1 & a_2 b_2 & a_2 b_3 \\
		a_3 b_1 & a_3 b_2 & a_3 b_3
	\end{pmatrix} && \text{(Outer product)} \\
	\begin{pmatrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \otimes
	\begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} &=
	\begin{pmatrix}
		a_{11} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} &
		a_{12} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} \\
		a_{21} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} &
		a_{22} \begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix}
	\end{pmatrix} && \text{(Tensor product)}\\ &= 
	\begin{pmatrix}
		a_{11} b_{11} & a_{11} b_{12} & a_{12} b_{11} & a_{12} b_{12} \\
		a_{11} b_{21} & a_{11} b_{22} & a_{12} b_{21} & a_{12} b_{22} \\
		a_{21} b_{11} & a_{21} b_{12} & a_{22} b_{11} & a_{22} b_{12} \\
		a_{21} b_{21} & a_{21} b_{22} & a_{22} b_{21} & a_{22} b_{22}
	\end{pmatrix}
\end{align*}
This way, two vectors $\vec x$, $\vec y$ can be bound without losing any information. In contrast to just stacking the two vectors (which would also not incur any information loss), the tensor product has some nice mathematical properties, and can, as outlined by Smolensky in his paper, to a degree be easily implemented in a neural substrate.
\begin{itemize}
	\item[\OPlus] Solves the binding problem and the problem of two.
	\item[\OMeh] Unclear how to solve Jackendoff's fourth challenge.
	\item[\OMinus] The method scales extremely poorly. Every time two vectors are bound, the dimensionality of the resulting structure is squared; we need $d^2$ dimensions when binding two $d$-dimensional vectors. In general, for $n$ binding operations we need $d^n$ dimensions.
\end{itemize}

\Note{\emph{Symbolic Architectures and Neuroscience.} All methods discussed so far are trying very hard to map purely symbolic architectures onto a neural substrate. In a sense, neural aspects are treated as \emph{mere implementation details}. This is an instance of the top-down approach we discussed at beginning of the course: mapping high-level cognitive architectures onto biology. In a sense, the hope is that, if successful, neurons would not matter. This is (unfortunately) an assumption many cognitive scientists make.}

\section{Vector Symbolic Algebras}

The last idea---using vectors $\vec x$ to represent symbols and to then use an operator such as $\otimes$ to bind them---seems to be reasonable, and fits the Neural Engineering Framework quite well. However, what we would like to have is an operator that maintains the dimensionality of the vectors (i.e., takes two vectors of dimensionality $d$ as an input and outputs a vector of dimensionality $d$), while still allowing us to reconstruct information about the operands.

\Example{\emph{Using \enquote{$+$} as a binding operator}.
Let's reconsider the \enquote{Binding Problem} as originally posed above. We would like to represent the fact that we are seeing a blue square and a red circle. Hence, we could---for now, randomly---generate four $d$-dimensional vector representing these four concepts, where $d$ is relatively large, for example $d = 64$. 

To keep us from having to come up with letters $\vec x$, $\vec y$, $\vec z$, $\vec w$, \textellipsis for each concept and remember the mapping between letter and concept, we will just write the vector corresponding to each concept in capital letters, like so: $\Obj{BLUE}$, $\Obj{RED}$, $\Obj{CIRCLE}$, $\Obj{SQUARE}$. Keep in mind that this is just notation; each of these words is a (random), $d$-dimensional vector.

Our first attempt at representing the above concept could be to just sum these vectors
\begin{align*}
	\vec x = \Obj{BLUE} + \Obj{SQUARE} + \Obj{RED} + \Obj{CIRCLE} \,.
\end{align*}
However, notice that mere addition of symbol vectors does not allow us to distinguish which colour belongs to which object. Correspondingly, \enquote{$+$} is not a good choice as a binding operator, but may be used to represent that two separate concepts are currently active.}

\newcommand{\CC}{\circledast}
Mathematically, what we would like is an operator $\CC$ with the following properties
\begin{align*}
	\CC &: \mathbb{R}^d \times \mathbb{R}^d \longrightarrow \mathbb{R}^d \,, && \text{(preservation of dimensionality)} \\
	\vec x &\approx (\vec x \CC \vec y) \CC \vec y^{-1} \,, && \text{(approximately reversible)} \\
	0 &\approx \langle \vec x \CC \vec y, \vec x \rangle \,, 0 \approx \langle \vec x \CC \vec y, \vec y \rangle \,. && \text{(dissimilar to inputs)}
\end{align*}
As we have discussed in the above example, the last property ensures that two concepts that are bound to each other cannot be confused with the original concepts. This prevents us from using an operator such as \enquote{$+$} as a binding operator, which preserves similarity, as graphically depicted in \cref{fig:cconv_sim}, and as can be easily shown:
\begin{align*}
	\langle \vec x + \vec y, \vec x \rangle = \langle \vec x, \vec x \rangle + \langle \vec x, \vec y \rangle \approx \|\vec x\|^2 + 0 \,,
\end{align*}
assuming that $\vec x$ and $\vec y$ are two high-dimensional random vectors, which are likely to be orthogonal. We will still use \enquote{$+$} to combine multiple concepts into one symbolic vector.

The use of vectors $\vec x$ along with a binding operator $\otimes$ to construct cognitive symbolic representations have been called \enquote{Vector Symbolic Architectures} by Ross Gayler in 2003 \cite{gayler2003vector}. More properly, these are \emph{algebras}, not architectures, so we adopt that terminology here. In the his paper, Gayler argues that such algebras can be used to solve all four challenges put forward by Jackendoff.

\begin{figure}
	\centering
	\includegraphics{media/cconv_sim.pdf}
	\caption{Similarity preservation and approximate reversibility of circular convolution. The $+$ operator preserves similarity between $\vec x$, $\vec y$ (both the penguin and the cat are still visible), whereas circular convolution $\circledast$ does not (neither the penguin nor the cat are visible). Convolving $\vec x \circledast \vec y$ with the pseudo-inverse $\vec x^{-1}$ and $\vec y^{-1}$ creates an image that is more similar to $\vec y$ and $\vec x$, respectively. The use of images in this example does not imply that we usually apply these methods to uncompressed images. \CodeLink{lecture_10/media/code/cconv_similarity.ipynb}}
	\label{fig:cconv_sim}
\end{figure}

\newpage
\Example{\emph{Possible Binding Operators.} Among others, the following binding operators have been proposed for use in vector symbolic algebras:
\begin{align*}
	\begin{pmatrix}1 \\ 0 \\ 1 \\ 0\end{pmatrix} \oplus \begin{pmatrix}1 \\ 1 \\ 0 \\ 0\end{pmatrix} &= \begin{pmatrix}0 \\ 1 \\ 1 \\ 0\end{pmatrix} && \textit{(XOR)}\\
	\begin{pmatrix}A \\ B \\ C \\ D\end{pmatrix} \odot \begin{pmatrix}E \\ F \\ G \\ H\end{pmatrix} &= \begin{pmatrix}AE \\ BF \\ CG \\ DH\end{pmatrix} && \textit{(Hadamard Product)} \\
	\begin{pmatrix}A \\ B \\ C \\ D\end{pmatrix} \CC \begin{pmatrix}E \\ F \\ G \\ H\end{pmatrix} &= \begin{pmatrix}AE &+& BH &+& CG &+& DF \\ AF &+& BE &+& CH &+& DG \\ AG &+& BF &+& CE &+& DH \\ AH &+& BG &+& CF &+& DE\end{pmatrix} && \textit{(Circular Convolution)}
\end{align*}
Circular convolution compresses the outer product by summation along the diagonals:\\[0.25cm]
\hspace*{1.315cm}\includegraphics{media/cconv_outer_product.pdf}
%\begin{align*}
%	\begin{pmatrix}A \\ B \\ C \\ D\end{pmatrix} \otimes \begin{pmatrix}E \\ F \\ G \\ H\end{pmatrix} &=
%	\begin{pmatrix}AE & \vphantom{+} & AF & \vphantom{+} & AG & \vphantom{+} & AH\\
%	               BE & \vphantom{+} & BF & \vphantom{+} & BG & \vphantom{+} & BH\\
%                   CE & \vphantom{+} & CF & \vphantom{+} & CG & \vphantom{+} & CH\\
%                   DE & \vphantom{+} & DF & \vphantom{+} & DG & \vphantom{+} & DH\end{pmatrix} && \textit{(Outer Product)}
%\end{align*}
}


\subsection{Example: Encoding Sentences}
We can use the binding operator \enquote{$\circledast$} and the plus operator \enquote{$+$} to compress multiple symbols into a single vector. For example, we can now write the above example as
\begin{align*}
	\vec x = \Obj{BLUE} \CC \Obj{SQUARE} + \Obj{RED} \CC \Obj{CIRCLE} \,.
\end{align*}
We can use the reversibility property to \enquote{ask questions}. For example, \enquote{which object is blue?}
\begin{align*}
	\vec y &= \big( \Obj{BLUE} \CC \Obj{SQUARE} + \Obj{RED} \CC \Obj{CIRCLE} \big) \CC \Obj{BLUE}^{-1} \\
		   &= \big( \Obj{BLUE} \CC \Obj{SQUARE} \big) \CC \Obj{BLUE}^{-1} + \big( \Obj{RED} \CC \Obj{CIRCLE} \big) \CC \Obj{BLUE}^{-1} \\
		   &\approx \Obj{SQUARE} + \underbrace{\Obj{RED} \CC \Obj{CIRCLE} \CC \Obj{BLUE}^{-1}}_{\text{\enquote{noise}}} 
		    \approx \Obj{SQUARE} \,.
\end{align*}
Note that we call the term $\Obj{RED} \CC \Obj{CIRCLE} \CC \Obj{BLUE}^{-1}$ \enquote{noise} because it does not correspond to any meaningful symbol without our vocabulary. Correspondingly, due to the linearity of addition, we know that the vector $\approx \Obj{SQUARE} + \text{\enquote{noise}}$ is highly similar to $\Obj{SQUARE}$, but to no other symbol in our vocabulary---this is what we mean by the last approximate equality in the above equation.

We can use this technique to write down more interesting concepts, such as the sentences we talked about in the previous section:
\begin{itemize}
	\item \enquote{The number nine comes after the number eight}:
	\begin{align*}
		\Obj{NUMBER} \CC \Obj{NINE} + \Obj{SUCC} \CC \Obj{EIGHT} \,.
	\end{align*}
	\item \enquote{The dog chases the cat}:
	\begin{align*}
		\Obj{DOG} \CC \Obj{SUBJ} + \Obj{CAT} \CC \Obj{OBJ} + \Obj{CHASE} \CC \Obj{VERB} \,.
	\end{align*}
	\item \enquote{Anne knows that Bill thinks that Charlie likes Dave}:
	\begin{align*}
	 	\Obj{SUBJ} \CC \Obj{ANNE} + \Obj{ACT} \CC \Obj{KNOWS} + \Obj{OBJ} \CC \\\Big(\Obj{SUBJ} \CC \Obj{BILL} + \Obj{ACT} \CC \Obj{THINKS} + \Obj{OBJ} \CC \\ \big(\Obj{SUBJ} \CC \Obj{CHARLIE} + \Obj{ACT} \CC \Obj{LIKES} + \Obj{OBJ} \CC \Obj{DAVE}\big)\Big) \,.
	\end{align*}
\end{itemize}

\Note{\emph{Graceful degradation.}
	The information we can pack into a single vector $\vec x$ is limited! The larger the number of binding operations, and the number of additions, the lower the precision of operations such as the approximate inverse. While this may sound bad, humans have similar limitations: for example, the nesting depth of sentences, or the number of objects we can keep in working memory is limited. This makes for interesting predictions if we use VSAs to model cognitive phenomena.}

\subsection{Circular Convolution}

In the following, we are going to focus on \emph{circular convolution} as a binding operator. Circular convolution has been used in signal processing for a long time, but has been proposed by Tony Plate in the 1990s as a binding operator \cite{plate1995holographic} and is used extensively in SPA models including Spaun. Circular convolution of two vectors $\vec z = \vec x \CC \vec y$ is defined as
\begin{ImportantEqn}{Circular Convolution}
	z_i &= \sum_{j = 0}^{d - 1} x_{j} y_{i - j \hspace{-1.25mm} \mod d } \,, & \text{ where } \vec x, \vec y, \vec z \in \mathbb{R}^d \,, \text{ and } a \hspace{-2mm} \mod b = a - b \left\lfloor \frac{a}b \right\rfloor \,.
	\label{eqn:cc}
\end{ImportantEqn}
This operator has the properties we demanded above. A pseudo-inverse $\vec x^{-1}$ according to the above definition is given as
\begin{align*}
 	\begin{pmatrix} x_0 & x_1 & x_2 & \hdots & x_{d - 2} & x_{d - 1} \end{pmatrix}^{-1} &= \begin{pmatrix} x_0 & x_{d-1} & x_{d-2} & \hdots & x_2 & x_1 \end{pmatrix}^{-1} \,,
\end{align*}
i.e., all elements except for the first one are reversed in order.

\Note{For most vectors, one can also solve for a more precise inverse by solving a least-squares problem, i.e., minimizing the error $\| \vec y \CC \vec x \CC \vec x^{-1} - \vec y\|^2$. However, the pseudo-inverse as defined above is sufficient in most cases.}

One potential problem with circular convolution as defined in \cref{eqn:cc}, especially with respect using it in conjunction with the NEF, is that the operator requires $d^2$ multiplications of vector coefficients. This means, that, if, for example, $d = 256$, then we need to perform $65536$ multiplications. This is a little concerning, particularly because even just multiplying a single pair of scalars with a reasonably small error requires on the order of one hundred pre-neurons. Correspondingly, we would end up with requiring about $6\times10^6$ neurons just for convolving two semantic pointers.

Luckily, just like \enquote{normal} convolution, circular convolution can be expressed as a simple multiplication in the Fourier domain
\begin{align*}
	\vec z &= \mathcal{DFT}^{-1} \big( \mathcal{DFT}(\vec x) \odot \mathcal{DFT}(\vec y) \big) \,,
\end{align*}
where $\odot$ is element-wise multiplication, or the so-called \enquote{Hadamard} product. Recall that the discrete and inverse discrete Fourier transformation are just linear basis transformations, so we can rewrite the above equation as
\begin{align*}
	\vec z &= \mat T^{-1} \big( \mat T \vec x \odot \mat T \vec y \big) \,,
\end{align*}
where $\mat T$ is the constant (complex-valued) matrix describing the Fourier transformation for a $d$-dimensional vector. As discussed in the lecture about transformations, we can easily compute any linear transformation $\mat T$ in the connection between two neuron populations by simply multiplying the decoder $\mat D$ with $\mat T$. Hence, all we are left with is $d$ complex-valued multiplications. Circular convolution is implemented in \texttt{nengo.networks.CircularConvolution} as an efficient NEF network.

\subsection{Example: Encoding Numbers}

Another fun example is encoding integers. Suppose that we have a symbol $\Obj{ONE}$ representing the number one. Then, we can define integers as repeated binding of this vector with itself
\begin{align*}
	\Obj{TWO} &= \Obj{ONE} \CC \Obj{ONE} \,, \\
	\Obj{THREE} &= \Obj{ONE} \CC \Obj{TWO} = \Obj{ONE} \CC \Obj{ONE} \CC \Obj{ONE} \,, \\
	\Obj{NUMBER\text{-$k$}} &= \underbrace{\Obj{ONE} \CC \Obj{ONE} \CC \text{\textellipsis} \CC \Obj{ONE}}_{\text{$k$-times}} \,.
\end{align*}
Using what we know about the relationship between circular convolution and the discrete Fourier transformation, we can also write the above as
\begin{align*}
	\Obj{NUMBER\text{-$k$}} &= \mathcal{DFT}^{-1} \big( \mathcal{DFT}(\Obj{ONE})^k \big) \,,
\end{align*}
where exponentiation operator \enquote{$\cdot^k$} corresponds to element-wise exponentiation.

\Note{\emph{Representing continuous values as symbol vectors.}
Notice that this allows us to represent any continuous scalar $k$, and not just integers! We will come back to this when we discuss spatial semantic pointers.}

\Note{\emph{Unitary vectors.}
One potential problem with this approach is that the magnitude of the vector $\vec x$ might exponentially increase or decrease with repeated binding. We thus want binding by $\Obj{ONE}$ to preserve inner products. A symbol vector $\vec x$ with this property is called a \emph{unitary} vector. \enquote{Unitary} because binding with this vector results in \enquote{unit} scaling (i.e., no scaling, or scaling by a factor of one). Mathematically, a unitary vector is a vector $\vec x$ with the following property
\begin{align*}
	\| \vec y \CC \vec x \| = \| \vec y \| \text{ for all } \vec y \in \mathbb{R}^d \,.
\end{align*}}

\subsection{Example: Raven's Progressive Matrices}

\begin{figure}[t]
	\centering%
	\begin{subfigure}{0.33\textwidth}%
		\centering%
		\includegraphics[width=0.9\textwidth]{media/ravens_example_a.pdf}%
		\caption{}%
		\label{fig:ravens_example_a}%
	\end{subfigure}%
	\begin{subfigure}{0.33\textwidth}%
		\centering%
		\includegraphics[width=0.9\textwidth]{media/ravens_example_b.pdf}%
		\caption{}%
		\label{fig:ravens_example_b}%
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}%
		\centering%
		\includegraphics[width=0.9\textwidth]{media/ravens_example_c.pdf}%
		\caption{}%
		\label{fig:ravens_example_c}%
	\end{subfigure}
	\caption{Examples similar to Raven's Progressive Matrices. Participants are asked to select one of eight possible answers that completes the matrix according to the rules underlying its construction.}
	\label{fig:ravens_example}
\end{figure}

Raven's progressive matrices is a commonly used IQ test developed in 1936. Questions on the test typically consist of $3 \times 3$ matrices filled with symbols arranged according to an unknown, \enquote{hidden} rule. One of the cells in the matrix is empty, and participants are asked to complete the matrix according to the rule by choosing one of eight possible cells. \Cref{fig:ravens_example} shows two examples of what the test could conceptually look like (these are not actual questions on the test; the actual test is kept secret and has not changed since 1936).

Perhaps surprisingly, this IQ test can be solved with a relatively high precision using Vector Symbolic Algebras. To this end, consider the example in \cref{fig:ravens_example_c} and assume that we already have a vision system that translates each cell into a corresponding vector representation. We can argue that this is a problem that we could solve really well using a modern convolutional neural network.

Given this translation between vision and symbol vector, each of the eight given cells could be described in the following way:
\begin{align*}
	\Obj{C1} &= \Obj{ONE}  \CC \Obj{P1}                                                       \,, &
	\Obj{C5} &= \Obj{FOUR} \CC \Obj{P1} + \Obj{FOUR} \CC \Obj{P2}                             \,, \\
	\Obj{C2} &= \Obj{ONE}  \CC \Obj{P1} + \Obj{ONE}  \CC \Obj{P2}                             \,, &
	\Obj{C6} &= \Obj{FOUR} \CC \Obj{P1} + \Obj{FOUR} \CC \Obj{P2} + \Obj{FOUR} \CC \Obj{P3}   \,, \\
	\Obj{C3} &= \Obj{ONE}  \CC \Obj{P1} + \Obj{ONE}  \CC \Obj{P2} + \Obj{ONE}  \CC \Obj{P3}   \,, &
	\Obj{C7} &= \Obj{FIVE} \CC \Obj{P1}                                                       \,, \\
	\Obj{C4} &= \Obj{FOUR}   \,, &
	\Obj{C8} &= \Obj{FIVE} \CC \Obj{P1} + \Obj{FIVE} \CC \Obj{P2}                             \,.
\end{align*}
We can then extract the hidden \enquote{rule}, or \enquote{transformation} between the cells using the pseudo-inverse (here, we're only looking at the \enquote{horizontal} rule from one cell to the next)
\begin{align*}
	\Obj{T1} &= \Obj{C2} \CC \Obj{C1}^{-1} \,, &
	\Obj{T4} &= \Obj{C6} \CC \Obj{C5}^{-1} \,, \\
	\Obj{T2} &= \Obj{C3} \CC \Obj{C2}^{-1} \,, &
	\Obj{T5} &= \Obj{C8} \CC \Obj{C7}^{-1} \,, \\
	\Obj{T3} &= \Obj{C5} \CC \Obj{C4}^{-1} \,.
\end{align*}
Assuming the rule is consistent, we should be able to extract a clean version of the rule by just averaging these five vectors:
\begin{align*}
	\Obj{T} &= \frac{\Obj{T1} + \Obj{T2} + \Obj{T3} + \Obj{T4} + \Obj{T5}}5 \,.
\end{align*}
Then, we can make a prediction as for what the ninth cell should contain:
\begin{align*}
	\Obj{C9} &= \Obj{C8} \CC \Obj{T} \approx \Obj{FIVE}  \CC \Obj{P1} + \Obj{FIVE}  \CC \Obj{P2} + \Obj{FIVE}  \CC \Obj{P3} \,.
\end{align*}
\pagebreak
\Note{For more detail on the Raven's Progressive Matrices particular example, see \cite{rasmussen2014spikinga}. Also, have a look at this video showing Spaun solving this task: \YouTube{Q_LRvnwnYp8}. This is based on a memory system showing both recency and primacy effects observed in humans: \YouTube{U_Q6Xjz9QHg}.}

\subsection{Revisiting Jackendoff's Challenges}

The above example demonstrated that we can solve cognitively challenging tasks using vector symbolic algebras (VSAs). Before we extend VSAs into the semantic pointer architecture (SPA) in the next lecture, let's revisit Jackendoff's challenges, and discuss in how far VSAs are able to solve these.

\begin{itemize}
	\item \textbf{The Binding Problem.} We already discussed this in the example above. In short, if we have concepts such as $\Obj{RED}$, $\Obj{BLUE}$, $\Obj{SQUARE}$, and $\Obj{CIRCLE}$, we can use the binding operator $\CC$ to bind concepts, and the addition operator $+$ for concepts that are active at the same time. For example, \enquote{red square and blue circle} becomes
	\begin{align*}
		\Obj{RED} \CC \Obj{SQUARE} + \Obj{BLUE} \CC \Obj{CIRCLE} \,.
	\end{align*}
	\item \textbf{The Problem of Two.} This problem is concerned with the question of how the same concept can be active at the same time in two different contexts. We also had a glance at a solution to this particular problem above: we can simply use symbols denoting the role of individual concepts. Consider the sentence \enquote{the little star is besides the big star}. We can express this in the following way:
	\begin{align*}
		\Obj{OBJ1} \CC (\Obj{TYPE} \CC \Obj{STAR} + \Obj{SIZE} \CC \Obj{LITTLE}) + \Obj{OBJ2} \CC (\Obj{TYPE} \CC \Obj{STAR} + \Obj{SIZE} \CC \Obj{BIG}) + \Obj{REL} \CC \Obj{BESIDES}
	\end{align*}
	\item \textbf{The Problem of Variables.} Here the problem was that language seems to be governed by abstract rules using placeholders. For example, we know that the adjective $\Obj{RED}$ can only be followed by a noun. This rule could be expressed as
	\begin{align*}
		\Obj{RULE} &= \Obj{RED} \CC \Obj{NOUN} \,.
	\end{align*}
	Then, a variable that can be substituted into the position of \enquote{$\Obj{NOUN}$} is given as
	\begin{align*}
		\Obj{VAR} &= \Obj{BALL} \CC \Obj{NOUN}^{-1} \,.
	\end{align*}
	Binding the variable with the rule results in the desired concept:
	\begin{align*}
		\Obj{RULE} \CC \Obj{VAR} &= ( \Obj{RED} \CC \Obj{NOUN} ) \CC ( \Obj{BALL} \CC \Obj{NOUN}^{-1} ) \\
		                         &= \Obj{RED} \CC \Obj{BALL} \CC ( \Obj{NOUN} \CC \Obj{NOUN}^{-1} ) \\
		                         &\approx \Obj{RED} \CC \Obj{BALL} \,.
	\end{align*}
	Note that we did not demand binding operators $\CC$ to be commutative in general; however, circular convolution happens to be commutative. For the above to work with a non-commutative binding operator, we can define $\Obj{VAR}$ as $\Obj{NOUN}^{-1} \CC \Obj{BALL}$.
	\item \textbf{Working vs.~Long-Term Memory.} This problem can be solved quite easily by considering vector symbolic algebras implemented on top of the NEF. Neural activities correspond to concepts that are currently in working memory. Long term memory corresponds to synaptic weights, and hence to functions transforming these concepts---for example, one possible function is an associative long-term memory.
\end{itemize}

\printbibliography

\end{document}

